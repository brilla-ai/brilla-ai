{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSMQ - Kwame AI Project\n",
    "\n",
    "## Script Title: HTML Parsing for Textbook Content Structuring\n",
    "### Author: Ernest Samuel\n",
    "#### Team: Data Preprocessing Team\n",
    "##### Date: June 24, 2023\n",
    "##### Updated: August 25, 2023\n",
    "\n",
    "---\n",
    "\n",
    "## Data Processing Functions\n",
    "\n",
    "This Jupyter Notebook encompasses a collection of five essential functions designed to streamline the extraction and structuring of content from HTML sources:\n",
    "\n",
    "1. **unique(array):** This function efficiently eliminates duplicate items from a processed dataset. It is designed to operate on a list of items, ensuring the final dataset remains free of redundancies.\n",
    "\n",
    "2. **extract_rawTable_of_content(link, homePage):** With this function, given a URL (referred to as 'link') and a specific page identifier (referred to as 'homePage'), the script extracts the table of contents of the textbook from the targeted website.\n",
    "\n",
    "3. **extract_url(link, pageList, maxNmber, char):** This function assembles URLs by concatenating the link with content-specific page identifiers extracted from the table of contents. The parameters include:\n",
    "   - `link`: The base URL up to the last forward slash\n",
    "   - `pageList`: A list of table of contents items\n",
    "   - `maxNmber`: The highest numerical index on the table of contents\n",
    "   - `char` (Optional): A list of non-numerical index characters\n",
    "   \n",
    "4. **extract_url_content(url, file_name):** Taking the URL of a textbook and an optional file name, this function extracts and structures the content from the URL, saving it as a JSON file.\n",
    "\n",
    "5. **extract_textbook(url_list, textbook_name):** This iterative function employs the list of URLs generated by function 3 to extract content. It uses function 4 to process and structure the content, ultimately saving it as a JSON file named after the textbook.\n",
    "\n",
    "6. **json_to_txt_script_notebook_.ipynb:** This Jupyter Notebook script, available in the same directory, provides a function `convert_to_txt_or_csv(input_filename, output_format)` to convert JSON content to either .txt or .csv format based on specified preferences.\n",
    "\n",
    "---\n",
    "\n",
    "This documentation outlines the purpose and functionality of each function, ensuring clarity and readability for future users and collaborators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Install, if missing, and import libraries ----------------------- #\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    print(\"Installing requests...\")\n",
    "    %pip install requests\n",
    "    import requests\n",
    "\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ImportError:\n",
    "    print(\"Installing beautifulsoup4...\")\n",
    "    %pip install beautifulsoup4\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    print(\"Installing pandas...\")\n",
    "    %pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "\n",
    "#------------ ------------ call .csv or .txt convertion Script ---------------------------------------#\n",
    "%run json_to_txt_script_notebook_.ipynb \n",
    "# --------------------------------------------------------------------------------#\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(array):\n",
    "    \"\"\"\n",
    "    Remove duplicates from a list while preserving the order of elements.\n",
    "    \n",
    "    Args:\n",
    "        array (list): The input list containing elements.\n",
    "        \n",
    "    Returns:\n",
    "        list: A new list with duplicates removed.\n",
    "    \"\"\"\n",
    "    return list(dict.fromkeys(array))\n",
    "\n",
    "def remove_items(superset_of_item, set_of_item):\n",
    "    \"\"\"\n",
    "    Remove items in list 'set_of_item' from list 'superset_of_item' and return the filtered list.\n",
    "    \n",
    "    This function filters out a subset 'set_of_item' from a superset 'superset_of_item' list.\n",
    "    It is used to exclude specific items from a list based on another list.\n",
    "    \n",
    "    Args:\n",
    "        superset_of_item (list): The superset list.\n",
    "        set_of_item (list): The subset list to be removed from 'superset_of_item'.\n",
    "        \n",
    "    Returns:\n",
    "        list: A new list with items from 'set_of_item' removed from 'superset_of_item'.\n",
    "    \"\"\"\n",
    "    superset_of_item = [item for item in superset_of_item if item not in set_of_item]\n",
    "    return superset_of_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Raw Table of Contents as URLs from Online Textbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rawTable_of_content(link, homePage):\n",
    "    \"\"\"\n",
    "    Extracts URLs of chapters from the table of contents of an online textbook.\n",
    "\n",
    "    Args:\n",
    "        link (str): Base URL of the textbook pages.\n",
    "        homePage (str): First landing page of the online view of the textbook.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique URLs representing chapters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the complete URL for the table of contents page\n",
    "    website_link = link + homePage\n",
    "    url_list = []\n",
    "\n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(website_link)\n",
    "    \n",
    "    # Check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the table of contents div using its class\n",
    "        table_of_contents_div = soup.find('div')\n",
    "\n",
    "        if table_of_contents_div:\n",
    "            # Find all the <a> tags within the table of contents div\n",
    "            a_tags = table_of_contents_div.find_all('a')\n",
    "\n",
    "            # Extract the href attribute from each <a> tag and store it in the list\n",
    "            for a_tag in a_tags:\n",
    "                href = a_tag.get('href')\n",
    "                url_list.append(href)\n",
    "        else:\n",
    "            print(\"Table of contents div not found on the website.\")\n",
    "            \n",
    "    # Ensure uniqueness of URLs in the list\n",
    "    return unique(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract each pages URL for required page of the textbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- From Table of Contents as URL, Extract pages URL for each required page of the textbook ----------------#\n",
    "\n",
    "def extract_url(link, pageList, maxNmber, alphabet_char=[]):\n",
    "    \"\"\"\n",
    "    Extracts URLs for specific pages of an online textbook.\n",
    "\n",
    "    Args:\n",
    "        link (str): Base URL of the textbook pages.\n",
    "        pageList (list): List of landing pages for specific chapters.\n",
    "        maxNmber (int): Maximum number of index numbers of the table of content.\n",
    "        alphabet_char (list, optional): List of alphabet characters or string indices. Default is an empty list.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique URLs representing specified pages.\n",
    "    \"\"\"\n",
    "    # Generate a combined list of numeric and alphabet characters\n",
    "    list_pages = list(range(1, maxNmber + 1))\n",
    "    list_pages.extend(alphabet_char)\n",
    "\n",
    "    url = []\n",
    "    \n",
    "    # Iterate through the provided pageList\n",
    "    for item in pageList:\n",
    "        for value in list_pages:\n",
    "            # Check if the item starts with the current value (number or alphabet)\n",
    "            if item.startswith(str(value)):\n",
    "                url.append(link + item)\n",
    "\n",
    "    # Ensure uniqueness of URLs in the list\n",
    "    return unique(url)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract contents from a URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_content(url):\n",
    "    \"\"\"\n",
    "    Extracts content from a URL's HTML structure, organized into sections, irrelevant_content, paragraphs, lists, figures, and tables.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the webpage to extract content from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing two dictionaries: irrelevant_content (chapter title and non-section paragraphs) \n",
    "              and content_list (structured sections, headings, paragraphs, lists, figures, and tables).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the main content section\n",
    "    main_content = soup.find(\"div\")\n",
    "\n",
    "    if main_content is None:\n",
    "        print(\"Unable to find the main content section\")\n",
    "        return\n",
    "\n",
    "    # List to store the content\n",
    "    content_list = []\n",
    "    \n",
    "    # Extract chapter title and non-section paragraphs\n",
    "    irrelevant_content = {}\n",
    "    head = soup.find('head')\n",
    "    Titles = head.find_all('title')\n",
    "    for title in Titles:\n",
    "        if title:\n",
    "            name = title.text.strip()\n",
    "            irrelevant_content[\"Title\"] = name\n",
    "\n",
    "    body = soup.find('body')\n",
    "    paras = []\n",
    "    pp = body.find_all('p')\n",
    "    for p in pp:\n",
    "        paras.append(p.text.strip())\n",
    "\n",
    "    # Find all the sections in the main content\n",
    "    sections = main_content.find_all(\"section\")\n",
    "\n",
    "    # Set to store unique section identifiers\n",
    "    section_identifiers = set()\n",
    "\n",
    "    # Iterate over each section\n",
    "    sec = []  # To generate a subset of paragraph data\n",
    "    for section in sections:\n",
    "        section_data = {}\n",
    "\n",
    "        # Extract section identifier\n",
    "        section_id = section.get(\"id\")\n",
    "        section_class = section.get(\"class\")\n",
    "        section_uuid_key = section.get(\"data-uuid-key\")\n",
    "        section_data_type = section.get(\"data-type\")\n",
    "        section_class_tuple = tuple(section_class) if section_class is not None else ()\n",
    "        section_identifier = (section_id, section_class_tuple, section_uuid_key, section_data_type)\n",
    "\n",
    "        # Skip if section identifier is already encountered\n",
    "        if section_identifier in section_identifiers:\n",
    "            continue\n",
    "\n",
    "        # Add section identifier to the set\n",
    "        section_identifiers.add(section_identifier)\n",
    "\n",
    "        # Extract section title\n",
    "        # ------------------------\n",
    "        subtitle = soup.find(['h3', 'h4', 'h2', 'h1'])\n",
    "        title = section.find([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\"])\n",
    "        if title:\n",
    "            section_data[\"title\"] = title.text.strip()\n",
    "        else:\n",
    "             section_data[\"title\"] = subtitle.text.strip()\n",
    "\n",
    "        # Extract section paragraphs\n",
    "        paragraphs = section.find_all([\"p\", \"span\"])\n",
    "        section_data[\"Section\"] = []\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            paragraph_text = paragraph.text.strip()\n",
    "            if paragraph_text:\n",
    "                section_data[\"Section\"].append(paragraph_text)\n",
    "                sec.append(paragraph_text)\n",
    "\n",
    "        # Extract list items\n",
    "        lists = section.find_all(\"ul\")\n",
    "        section_data[\"lists\"] = []\n",
    "        for ul in lists:\n",
    "            list_items = ul.find_all(\"li\")\n",
    "            section_data[\"lists\"].append([li.text.strip() for li in list_items])\n",
    "\n",
    "        # Extract figures and image links\n",
    "        figures = section.find_all(\"div\", {\"class\": \"os-figure\"})\n",
    "        section_data[\"figures\"] = []\n",
    "        for figure in figures:\n",
    "            figure_data = {}\n",
    "            img = figure.find(\"img\")\n",
    "           \n",
    "            if img and \"src\" in img.attrs:\n",
    "                image_url = urljoin(url, img[\"src\"])\n",
    "                figure_data[\"image\"] = image_url\n",
    "\n",
    "            caption = figure.find(\"figcaption\")\n",
    "            if caption:\n",
    "                figure_data[\"caption\"] = caption.text\n",
    "            \n",
    "            section_data[\"figures\"].append(figure_data)\n",
    "\n",
    "        # Extract tables\n",
    "        tables = section.find_all(\"table\")\n",
    "        section_data[\"tables\"] = []\n",
    "        for table in tables:\n",
    "            table_data = []\n",
    "            rows = table.find_all(\"tr\")\n",
    "            for row in rows:\n",
    "                cells = row.find_all(\"td\")\n",
    "                table_data.append([cell.text.strip() for cell in cells])\n",
    "            section_data[\"tables\"].append(table_data)\n",
    "\n",
    "        content_list.append(section_data)\n",
    "\n",
    "  # Extract only paragraphs that are not in the sections structure\n",
    "    irrelevant_content[\"Paragraphs_Not_in_Sections\"] = remove_items(paras, sec)\n",
    "\n",
    "    return [irrelevant_content, content_list]  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate and extract different pages, by URL, in the textbook and save as one file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_textbook(url_list, textbook_name):\n",
    "    \"\"\"\n",
    "    Extracts and structures content from a list of URLs into a JSON file.\n",
    "\n",
    "    Args:\n",
    "        url_list (list): List of URLs containing the textbook content.\n",
    "        textbook_name (str): Name of the textbook for JSON file naming.\n",
    "\n",
    "    Returns:\n",
    "        content_list: A list of dictionaries, containing structured content data.\n",
    "    \"\"\"\n",
    "    content_list = []  # List to store the structured content\n",
    "    page_data = {}  # Dictionary to store content per page\n",
    "    pages = 0  # Page counter\n",
    "    \n",
    "    # Iterate through the list of URLs\n",
    "    for url in url_list:\n",
    "        # Extract content from the URL using a helper function (extract_url_content)\n",
    "        page_content = extract_url_content(url)\n",
    "        \n",
    "        # The page_content is a list with two items:\n",
    "        # 1. irrelevant_content (not used in this context)\n",
    "        # 2. content_list (well-structured sections needed)\n",
    "        # We access content_list using indexing [1] to remove paragraphs not in sections since it is not needed for the training dataset.\n",
    "        \n",
    "        page_data['Page ' + str(pages)] = page_content[1]\n",
    "        pages += 1\n",
    "    \n",
    "    content_list.append(page_data)  # Append structured content to the list\n",
    "    \n",
    "    # Get the current working directory and create the file path for JSON\n",
    "    script_dir = os.getcwd()\n",
    "    json_path = os.path.join(script_dir, f\"{textbook_name}.json\")\n",
    "    \n",
    "    # Save the structured content as JSON\n",
    "    with open(json_path, \"w\") as file:\n",
    "        json.dump(content_list, file, indent=4)\n",
    "    \n",
    "    return content_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting content from: Biology for AP courses\n",
      "Biology for AP courses Data extracted and saved as .csv successfully in formatted_files.\n",
      "Extracting content from: Chemistry 2e\n",
      "Chemistry 2e Data extracted and saved as .csv successfully in formatted_files.\n",
      "Extracting content from: High School Physics\n",
      "High School Physics Data extracted and saved as .csv successfully in formatted_files.\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file containing links to the textbooks\n",
    "# CSV should have columns 'BOOKS' for textbook names and 'URL' for URLs\n",
    "\n",
    "openStax = pd.read_csv('openstax_textbooks_sheet.csv')\n",
    "\n",
    "# Select specific textbooks to extract\n",
    "selected_textbooks = openStax.iloc[[25,22,3]]\n",
    "\n",
    "# Iterate through selected textbooks and extract content\n",
    "for BookName, urls in zip(selected_textbooks['BOOKS'], selected_textbooks['URL']):\n",
    "   \n",
    "    print(\"Extracting content from:\", BookName)\n",
    "    \n",
    "    # Initialize the starting page for online view\n",
    "    landing_page = 'preface'\n",
    "    site = str(urls)\n",
    "\n",
    "    # Extract the table of contents\n",
    "    pageList = extract_rawTable_of_content(site, landing_page)\n",
    "\n",
    "    # Initialize the starting index for required content from the table of contents\n",
    "    max_number = 10  # Maximum number of numerical index on the table of contents\n",
    "    char = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'i']\n",
    "\n",
    "    # Extract the list of URLs to be parsed for website scraping (used in htmlProcessing)\n",
    "    URLs = extract_url(site, pageList, max_number, char)\n",
    "\n",
    "    # Name of the textbook\n",
    "    textbook = str(BookName)\n",
    "    \n",
    "    # Extract the textbook content\n",
    "    extracted_content = extract_textbook(URLs, textbook)\n",
    "    file_name = f\"{BookName}.json\"\n",
    "    output_format = \"csv\"\n",
    "    convert_to_txt_or_csv(file_name, output_format)  # check json_to_txt_script_notebook_.ipynb file, on how to use this function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert .json file to .txt or .csv file. \n",
    "- make sure `json_to_txt_script_notebook_.ipynb` is in thesame folder as this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "College Physics for AP Courses 2e Data extracted and saved as .csv successfully in formatted_files.\n"
     ]
    }
   ],
   "source": [
    "input_filename = 'College Physics for AP Courses 2e.json'  # Replace with the actual input file name\n",
    "output_format = \"csv\"               # Choose output format (\"csv\" or \"txt\")\n",
    "\n",
    "# Call the function with the input filename and output format, default is .txt\n",
    "convert_to_txt_or_csv(input_filename, output_format)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage Structure\n",
    "\n",
    "The stored data follows a structured organization aligned with the website's table of contents:\n",
    "\n",
    "- Data is organized using dictionary format, corresponding to each page of the website.\n",
    "- Each page contains titles and corresponding sections. Within each section, there are lists, figures, and tables.\n",
    "- Sections encompass paragraphs from the textbook.\n",
    "- Figures include links to images when present in a section.\n",
    "- Tables capture tabular data identified within a section.\n",
    "- Lists encompass ordered or unordered list items found in sections.\n",
    "\n",
    "## Notable Insights\n",
    "\n",
    "- When storing data in JSON format, certain special characters, such as mathematical equations, may be encoded.\n",
    "- Printing the content directly can reveal more about the format and structure.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
