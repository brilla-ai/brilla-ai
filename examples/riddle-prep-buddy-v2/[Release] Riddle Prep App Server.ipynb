{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBsImOKYbMFe"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C-YM1rzbF6a",
        "outputId": "8639aef1-edd6-40bc-dd2c-eaf5148e3b1d"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BG0fM3XCa451",
        "outputId": "1cce67fe-2b91-4ce8-f149-bdcb29539e65"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install TTS\n",
        "!sudo apt-get install espeak-ng\n",
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "\n",
        "# STT\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer\n",
        "!pip install tabulate\n",
        "!pip install pydub\n",
        "!pip install transformers\n",
        "\n",
        "# API-related dependencies\n",
        "!pip install fastapi uvicorn pydantic pyngrok nest_asyncio\n",
        "!pip install python-multipart\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoOhnH_hbauY"
      },
      "outputs": [],
      "source": [
        "# TTS-Related Imports\n",
        "import IPython\n",
        "import tempfile\n",
        "import subprocess\n",
        "import time\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.models.vits import Vits\n",
        "from TTS.utils.audio.numpy_transforms import save_wav\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# STT-Related Imports\n",
        "import io\n",
        "import wave\n",
        "import numpy as np\n",
        "import whisper\n",
        "import jiwer\n",
        "import time\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import joblib\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "# API-Related Imports\n",
        "from fastapi import FastAPI,Response\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from starlette.middleware.gzip import GZipMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.responses import StreamingResponse,FileResponse\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "import shutil\n",
        "from pydantic import BaseModel\n",
        "from IPython.display import Audio\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import base64\n",
        "\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfOuc8Pxg8Y_"
      },
      "outputs": [],
      "source": [
        "from starlette.middleware.gzip import GZipMiddleware\n",
        "app = FastAPI()\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "app.add_middleware(GZipMiddleware, minimum_size=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlS247Nggdnb"
      },
      "source": [
        "## TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYUzxcTsbg17"
      },
      "outputs": [],
      "source": [
        "# Load TTS model\n",
        "live_config=VitsConfig()\n",
        "live_config.load_json(\"/content/drive/MyDrive/NSMQ AI Project/Technical/TTS/Prof Elsie Kauffmann/VITS model/vits-elsie/traineroutput/vits_vctk-May-24-2023_11+05PM-23a7a9a3/config.json\")\n",
        "live_vits = Vits.init_from_config(live_config)\n",
        "live_vits.load_onnx(\"/content/drive/MyDrive/NSMQ AI Project/Technical/TTS/Prof Elsie Kauffmann/VITS model/vits-elsie/elsie.onnx\")\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NuZNL3ShiVG"
      },
      "outputs": [],
      "source": [
        "def live_audio(text:str):\n",
        "  text_inputs = np.asarray(\n",
        "      live_vits.tokenizer.text_to_ids(text, language=\"en\"),\n",
        "      dtype=np.int64,\n",
        "  )[None, :]\n",
        "  audio = live_vits.inference_onnx(text_inputs,speaker_id=0)\n",
        "  with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
        "    out_path = temp_file.name\n",
        "  save_wav(wav=audio[0], path=out_path,sample_rate=22050)\n",
        "  return out_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWQ0Ci43goyB"
      },
      "outputs": [],
      "source": [
        "class LiveText(BaseModel):\n",
        "  text: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhC41XNvhZxr"
      },
      "outputs": [],
      "source": [
        "@app.get('/synthesize-speech')\n",
        "def onnx_audio(payload:LiveText):\n",
        "  out_path=live_audio(payload.text)\n",
        "  return FileResponse(out_path, media_type=\"audio/wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYnYzhfaghmB"
      },
      "source": [
        "## STT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YiZLETpqBP_",
        "outputId": "3b89c522-0271-4189-fa3f-da1ec549f456"
      },
      "outputs": [],
      "source": [
        "# Load STT Model\n",
        "# Load whisper model\n",
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# DEVICE = \"cpu\"\n",
        "\n",
        "model = whisper.load_model(\"medium.en\", device = DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBgmmhkXqHOl"
      },
      "outputs": [],
      "source": [
        "def transcribe_audio(path_to_audio):\n",
        "  \"\"\"Loads whisper model to transcribe audio\"\"\"\n",
        "\n",
        "  # Load audio\n",
        "  audio = whisper.load_audio(path_to_audio)\n",
        "\n",
        "  # Transcribe audio\n",
        "  result = model.transcribe(audio)\n",
        "\n",
        "  # Print transcript\n",
        "  return result[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMFjRgyQrjrB"
      },
      "outputs": [],
      "source": [
        "class AudioBytes(BaseModel):\n",
        "  data: bytes\n",
        "  filename: str\n",
        "\n",
        "@app.get(\"/get-transcript\")\n",
        "async def get_transcript(audio: AudioBytes):\n",
        "  try:\n",
        "    decoded_data = base64.b64decode(audio.data)\n",
        "\n",
        "    # Write bytes data to a .wav file\n",
        "    with io.BytesIO(decoded_data) as audio_file:\n",
        "        with wave.open(audio_file, \"wb\") as wav:\n",
        "          wav.setnchannels(1)\n",
        "          wav.setsampwidth(2)\n",
        "          wav.setframerate(16000)\n",
        "\n",
        "          # Write .wav files\n",
        "          wav.writeframes(decoded_data)\n",
        "\n",
        "    # Save the audio file with the custom name\n",
        "    audio_filename = audio.filename\n",
        "    with open(audio_filename, \"wb\") as file:\n",
        "        file.write(decoded_data)\n",
        "\n",
        "    transcript = transcribe_audio(audio_filename)\n",
        "    os.remove(audio_filename)\n",
        "    return {\"transcript\": transcript}\n",
        "  except Exception as e:\n",
        "    return {\"error\":str(e)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1Bc0gtOeFRK",
        "outputId": "0cef57a3-2980-41e7-f0fd-00603c14156e"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken # TO DO: Replace this comment with your ngronk token (can be obtained from your ngronk account)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgncu44zisvo",
        "outputId": "17f58b0f-01d3-48ba-bb79-6c81c9e53a55"
      },
      "outputs": [],
      "source": [
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "uvicorn.run(app, port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
