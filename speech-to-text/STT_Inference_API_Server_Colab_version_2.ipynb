{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsmq-ai/nsmqai/blob/kojomensahonums-stt-inference-notebook-version_2/STT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phsd-uQ9skuK"
      },
      "outputs": [],
      "source": [
        "# Import and install the required libraries for asr\n",
        "\n",
        "%%capture\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer\n",
        "!pip install tabulate\n",
        "!pip install pydub\n",
        "!pip install transformers\n",
        "import torch\n",
        "import numpy as np\n",
        "import whisper\n",
        "import jiwer\n",
        "import time\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import joblib\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2xrHNXtLhSY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install required libraries for web api\n",
        "!pip  install fastapi\n",
        "!pip -q install pyngrok\n",
        "!pip -q install uvicorn\n",
        "!pip -q install nest_asyncio\n",
        "!pip -q install python-multipart\n",
        "# nest_asyncio.apply()\n",
        "#!pip uninstall fastapi typing-extensions -y\n",
        "#!pip install fastapi typing-extensions==4.8.0 --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MIeAXHiLjLm"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import uvicorn\n",
        "#from typing_extensions import Annotated, Doc\n",
        "from fastapi import FastAPI,Response\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pyngrok import ngrok\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "import shutil\n",
        "\n",
        "# Import models for serialisation/ deserialisation\n",
        "from pydantic import BaseModel\n",
        "import base64\n",
        "import io\n",
        "import wave\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# app.add_middleware(\n",
        "#     CORSMiddleware,\n",
        "#     allow_origins=['*'],\n",
        "#     allow_credentials=True,\n",
        "#     allow_methods=['*'],\n",
        "#     allow_headers=['*'],\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzf7vnunmRfV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eestkRyb-74m"
      },
      "source": [
        "### Miscellaneous functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gme-keulsuJ3"
      },
      "outputs": [],
      "source": [
        "# Load whisper model\n",
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = whisper.load_model(\"medium.en\", device = DEVICE) # Select whisper model size (tiny, base, small, medium, large)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEJ1_QYHsyW_"
      },
      "outputs": [],
      "source": [
        "def transcribe(path_to_audio):\n",
        "  \"\"\"Loads whisper model to transcribe audio\"\"\"\n",
        "\n",
        "  # Load audio\n",
        "  audio = whisper.load_audio(path_to_audio)\n",
        "\n",
        "  # Transcribe audio\n",
        "  result = model.transcribe(audio)\n",
        "\n",
        "  # Print transcript\n",
        "  return result[\"text\"]\n",
        "\n",
        "  ## Approach 2\n",
        "  ### This uses a more user-controlled processing technique than the former\n",
        "  # audio = whisper.load_audio(path_to_audio)\n",
        "  # audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "  # # Make log-Mel spectrogram and move to the same device as the model\n",
        "  # mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "  # # Decode the audio\n",
        "  # options = whisper.DecodingOptions(language= \"en\", without_timestamps= True, fp16 = False)\n",
        "  # result = whisper.decode(model, mel, options)\n",
        "\n",
        "  # return result.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQ8mMkqxJWIx"
      },
      "outputs": [],
      "source": [
        "def detect_start_point(transcribed_text):\n",
        "  \"\"\"Detects start points/ riddle cues present in audio transcripts\"\"\"\n",
        "\n",
        "  # Sample list of riddle start points\n",
        "  sample_start_points = [\"we begin\", \"i begin\", \"let's begin\",\\\n",
        "                         \"first riddle\", \"1st riddle\", \"riddle number one\", \"riddle number 1\",\\\n",
        "                         \"second riddle\", \"2nd riddle\", \"riddle number two\", \"riddle number 2\",\\\n",
        "                         \"third riddle\", \"3rd riddle\", \"riddle number three\", \"riddle number 3\",\\\n",
        "                         \"fourth riddle\", \"4th riddle\", \"riddle number four\", \"riddle number 4\",\\\n",
        "                         \"fifth riddle\", \"5th riddle\", \"riddle number five\", \"riddle number 5\",\\\n",
        "                         \"last riddle\", \"final riddle\", \"last one\", \"next one\", \"first one\", \\\n",
        "                         \"second one\", \"third one\", \"fourth one\", \"fifth one\",\\\n",
        "                         \"first redo\", \"second redo\", \"third redo\", \"fourth redo\", \"last redo\",\\\n",
        "                         \"final redo\", \"fifth redo\", \"fast riddle\"\n",
        "                         ]\n",
        "\n",
        "  # Check for a matching start point\n",
        "  matching_start_point = None\n",
        "  for start_point in sample_start_points:\n",
        "      if start_point in transcribed_text.lower():\n",
        "          matching_start_point = start_point\n",
        "          break\n",
        "\n",
        "  return matching_start_point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrebkTlVQVpa"
      },
      "outputs": [],
      "source": [
        "def detect_end_point(transcribed_text):\n",
        "  \"\"\"Detects end points present in audio transcripts\"\"\"\n",
        "\n",
        "  # Sample list of riddle start points\n",
        "  end_points = [\"who am i\"]\n",
        "\n",
        "  # Check for a matching start point\n",
        "  matching_end_point = None\n",
        "  for end_point in end_points:\n",
        "      if end_point in transcribed_text.lower():\n",
        "          matching_end_point = end_point\n",
        "          break\n",
        "\n",
        "  return matching_end_point"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KNj2eIhA0cTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3TVvsbldvIb"
      },
      "outputs": [],
      "source": [
        "class BertClassifier(nn.Module):\n",
        "  def __init__(self, pretrained_bert, num_classes):\n",
        "    super(BertClassifier, self).__init__()\n",
        "    self.model = pretrained_bert\n",
        "    self.input_size = self.model.config.hidden_size\n",
        "    # Fully connected classifier\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(self.input_size, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, num_classes)\n",
        "    )\n",
        "  def forward(self, input_ids, attention_mask,labels=None):\n",
        "      outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      embedding = outputs.last_hidden_state.mean(dim=1)  #mean pooling\n",
        "      logits = self.classifier(embedding)\n",
        "\n",
        "      if labels is not None:\n",
        "          loss_fn=nn.CrossEntropyLoss()\n",
        "          loss=loss_fn(logits,labels)\n",
        "          return loss\n",
        "      return logits\n",
        "\n",
        "def preprocess_bert_features(sentence,tokenizer):\n",
        "  tokenized_input = tokenizer.encode_plus(sentence,padding='max_length',  max_length=512,truncation=True, return_tensors='pt', )\n",
        "  return tokenized_input[\"input_ids\"], tokenized_input[\"attention_mask\"]\n",
        "\n",
        "def predict_clue(sentence, model, tokenizer):\n",
        "    # Preprocess the sentence\n",
        "    input_ids, attention_mask = preprocess_bert_features(sentence, tokenizer)\n",
        "\n",
        "    # Ensure tensors are on the same device as the model and perform inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
        "pretrained_bert_model = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
        "bert_model = BertClassifier(pretrained_bert=pretrained_bert_model, num_classes=2)\n",
        "bert_model.load_state_dict(torch.load('speech-to-text/bert_classifier_model.pth')) # custom-trained classifier model\n",
        "bert_model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2_y3MmBKs-q"
      },
      "outputs": [],
      "source": [
        "# Define global variables to store the accumulated clues and riddle clues\n",
        "accumulated_clues = []\n",
        "store_to_count = []\n",
        "\n",
        "def process_audio_chunk(audio_filename):\n",
        "  \"\"\"Performs final piece audio transcription and riddle clue concatenation for the QA model\"\"\"\n",
        "\n",
        "  # Initialize variables\n",
        "  transcribed_text = \" \"  # To store the concatenated text\n",
        "  previous_end_index = 0  # To keep track of the end index of the previous riddle\n",
        "  clue_counter = 0 # count clues per new line\n",
        "  is_new_riddle = False\n",
        "  end_of_clues = False\n",
        "\n",
        "  # Transcribe audio chunk\n",
        "  chunk_transcript = transcribe(audio_filename)\n",
        "\n",
        "  # Detect start point\n",
        "  start_point = detect_start_point(chunk_transcript)\n",
        "\n",
        "  # Detect end point\n",
        "  end_point = detect_end_point(chunk_transcript)\n",
        "\n",
        "  # If a matching start point is found, concatenate text\n",
        "  if start_point:\n",
        "    is_new_riddle = True\n",
        "    accumulated_clues.clear() # clear already stored clues\n",
        "    store_to_count.clear() # clear held count of riddle clues\n",
        "    start_index = chunk_transcript.lower().find(start_point.lower()) # identify first position of start-point phrase\n",
        "    previous_end_index = start_index + len(start_point) # set end position of start-point phrase\n",
        "\n",
        "  if end_point:\n",
        "    end_of_clues = True\n",
        "\n",
        "  # Add the transcribed chunk to the continuous text\n",
        "  transcribed_text = chunk_transcript[previous_end_index:].strip()\n",
        "\n",
        "  # Process the text for riddle clues\n",
        "  sentences = re.split(r'(?<=[.,?])', transcribed_text) # delimit transcribed text based on given punctuations\n",
        "  clues_found = [] # store identified clues\n",
        "\n",
        "  # Apply classifier on delimited texts in transcript\n",
        "  for sentence in sentences:\n",
        "    pred = predict_clue(sentence.strip(),bert_model,tokenizer) # applying classifier\n",
        "    if pred == 1:\n",
        "      accumulated_clues.append(sentence)  # Append the clue to the list\n",
        "      clues_found.append(sentence)\n",
        "\n",
        "  # Concatenate riddle clues\n",
        "  if clues_found:\n",
        "    grouped_clues = \" \".join(clues_found)\n",
        "    store_to_count.append(grouped_clues)\n",
        "    for i in range(len(store_to_count)):\n",
        "      clue_counter+=1\n",
        "    return chunk_transcript, \" \".join(accumulated_clues), clue_counter, is_new_riddle, end_of_clues #transcribed chunks, concatenated riddle clues, counter for clues, boolean if new riddle, boolean if riddle ends\n",
        "\n",
        "  return chunk_transcript, \" \", 0, is_new_riddle,  end_of_clues\n",
        "  clue_counter.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8OqQKrt69QU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sZlBqon_FrV"
      },
      "source": [
        "### For API Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jnwCjmhLBz3"
      },
      "outputs": [],
      "source": [
        "class AudioBytes(BaseModel):\n",
        "  data: bytes\n",
        "  filename: str\n",
        "\n",
        "@app.get(\"/get-transcript\")\n",
        "async def get_transcript(audio: AudioBytes):\n",
        "  try:\n",
        "    decoded_data = base64.b64decode(audio.data)\n",
        "\n",
        "    # Write bytes data to a .wav file\n",
        "    with io.BytesIO(decoded_data) as audio_file:\n",
        "        with wave.open(audio_file, \"wb\") as wav:\n",
        "          wav.setnchannels(1)\n",
        "          wav.setsampwidth(2)\n",
        "          wav.setframerate(16000)\n",
        "\n",
        "          # Write .wav files\n",
        "          wav.writeframes(decoded_data)\n",
        "\n",
        "    # Save the audio file with the custom name\n",
        "    audio_filename = audio.filename\n",
        "    with open(audio_filename, \"wb\") as file:\n",
        "        file.write(decoded_data)\n",
        "\n",
        "    # Get transcript and delete temporary audio file\n",
        "    chunk_transcript, current_clues, clue_counter, is_new_riddle, end_of_clues = process_audio_chunk(audio_filename) # current clues contains previous+recently identified clues\n",
        "    os.remove(audio_filename)\n",
        "    return {\"transcript\": chunk_transcript, \"clues\": current_clues, \"clue_count\":clue_counter, \"is_start_of_riddle\":is_new_riddle, \"is_end_of_riddle\":end_of_clues}\n",
        "  except Exception as e:\n",
        "    return {\"error\":str(e)}\n",
        "\n",
        "@app.get(\"/stt-test\")\n",
        "async def stt_test():\n",
        "  return {\"transcript\":\"Hello from STT.\", \"clues\":\"\", \"clue_count\":\"\", \"is_start_of_riddle\":\"\", \"is_end_of_riddle\":\"\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrh5RexfIrg7"
      },
      "outputs": [],
      "source": [
        "# Attach personal token\n",
        "#!ngrok config add-authtoken <place_your_ngrok_auth_token_here>\n",
        "!ngrok config add-authtoken # place_your_ngrok_auth_token_here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMR03ze9IKls"
      },
      "outputs": [],
      "source": [
        "# Create public url to access speech-to-text service\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwe7TuomF61F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
