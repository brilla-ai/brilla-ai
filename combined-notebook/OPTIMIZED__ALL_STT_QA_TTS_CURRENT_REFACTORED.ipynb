{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd53z9Iphmaz"
      },
      "source": [
        "# VARIABLES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAA4uc_Ghqlh"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# populate the values in the secrets section\n",
        "STT_WHISPER_MODEL = \"medium.en\"\n",
        "STT_BERT_MODEL = \"prajjwal1/bert-tiny\"\n",
        "STT_BERT_MODEL_DRIVE_LOCATION = userdata.get('STT_BERT_MODEL_DRIVE_LOCATION')\n",
        "QA_MODEL= \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "QA_LOG_DRIVE_LOCATION = userdata.get('QA_LOG_DRIVE_LOCATION')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "NGROK_TOKEN = userdata.get('NGROK_TOKEN')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIUbtlX_cCNL"
      },
      "source": [
        "# TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l43zhBtcOqIA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GUlDKw-NOqxy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install TTS\n",
        "!sudo apt-get install espeak-ng\n",
        "!pip install fastapi uvicorn pydantic pyngrok nest_asyncio\n",
        "!pip install python-multipart\n",
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "\n",
        "import IPython\n",
        "import tempfile\n",
        "import subprocess\n",
        "from fastapi import FastAPI,Response\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from starlette.middleware.gzip import GZipMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.responses import StreamingResponse,FileResponse\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "import shutil\n",
        "from pydantic import BaseModel\n",
        "from IPython.display import Audio\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import base64\n",
        "import time\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.models.vits import Vits\n",
        "from TTS.utils.audio.numpy_transforms import save_wav\n",
        "import numpy as np\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tE_yfR3dPIqN"
      },
      "outputs": [],
      "source": [
        "class OutputTTSText(BaseModel):\n",
        "  answer: str\n",
        "\n",
        "class DemoText(BaseModel):\n",
        "  text: str\n",
        "  voice: str\n",
        "\n",
        "class LiveText(BaseModel):\n",
        "  text: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j22B0W7KPMDg"
      },
      "outputs": [],
      "source": [
        "def demo_audio(text:str,voice:str):\n",
        "  if voice==\"1\":\n",
        "    text_inputs = np.asarray(\n",
        "        demo_vits.tokenizer.text_to_ids(text, language=\"en\"),\n",
        "        dtype=np.int64,\n",
        "    )[None, :]\n",
        "    audio = demo_vits.inference_onnx(text_inputs)\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
        "      out_path = temp_file.name\n",
        "    save_wav(wav=audio[0], path=out_path,sample_rate=22050)\n",
        "    return out_path\n",
        "  else:\n",
        "    text_inputs = np.asarray(\n",
        "        live_vits.tokenizer.text_to_ids(text, language=\"en\"),\n",
        "        dtype=np.int64,\n",
        "    )[None, :]\n",
        "    audio = live_vits.inference_onnx(text_inputs)\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
        "      out_path = temp_file.name\n",
        "    save_wav(wav=audio[0], path=out_path,sample_rate=22050)\n",
        "    return out_path\n",
        "\n",
        "def live_audio(text:str):\n",
        "  text_inputs = np.asarray(\n",
        "      live_vits.tokenizer.text_to_ids(text, language=\"en\"),\n",
        "      dtype=np.int64)[None, :]\n",
        "  audio = live_vits.inference_onnx(text_inputs,speaker_id=0)\n",
        "  with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
        "    out_path = temp_file.name\n",
        "  save_wav(wav=audio[0], path=out_path,sample_rate=22050)\n",
        "  return out_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEI9XF7Db_V3"
      },
      "source": [
        "# QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM769nSVRcMv"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "!pip install -q einops\n",
        "!pip install -q langchain\n",
        "!pip install -q bitsandbytes\n",
        "\n",
        "!pip install -q -U transformers accelerate\n",
        "# update or install the necessary libraries\n",
        "!pip install -q openai\n",
        "!pip install -q langchain-community\n",
        "!pip install -q python-dotenv\n",
        "\n",
        "# For API\n",
        "!pip -q install fastapi\n",
        "!pip -q install pyngrok\n",
        "!pip -q install uvicorn\n",
        "!pip -q install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x09rNht_RevF",
        "outputId": "ff30587f-92f9-4377-e36c-7dcb5d97a933"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import openai\n",
        "import os\n",
        "import IPython\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDCDAEln3BAc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SFpG2vQRh-x"
      },
      "outputs": [],
      "source": [
        "qa_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "qa_model = AutoModelForCausalLM.from_pretrained(qa_model_id, device_map=\"auto\")\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_id)\n",
        "\n",
        "\n",
        "pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=qa_model,\n",
        "        tokenizer=qa_tokenizer,\n",
        "        use_cache=True,\n",
        "        device_map=\"auto\",\n",
        "        max_length=500,\n",
        "        temperature = 1,\n",
        "        do_sample=True,\n",
        "        top_k=1,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=qa_tokenizer.eos_token_id,\n",
        "        pad_token_id=qa_tokenizer.eos_token_id,\n",
        "        torch_dtype=torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awsl2EvuRkyy"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 1.0})\n",
        "\n",
        "def answer_with_mistral(riddle):\n",
        "  template = \"\"\"  <s>[INST]   You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must first reason through the clues of the given riddle and then provide a short answer. Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "      Here's an example to guide you:\n",
        "      Riddle: You might think i am a rather unstable character because i never stay at one place. However my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time. I can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules. In order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency.\n",
        "      Answer: oscillator\n",
        "\n",
        "      Read the riddle below and provide the three possible correct answers as a json with keys: answer1, answer2, answer3\n",
        "\n",
        "      NOTE: You are allowed to include an answer multiple times if your reasoning shows that it is likely the correct answer. Do not provide any explanations.\n",
        "\n",
        "      Riddle: {riddle}\n",
        "\n",
        "      [/INST] </s>\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"riddle\"])\n",
        "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "  answer = llm_chain.run({\"riddle\":riddle})\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bCXcnfiRoP3"
      },
      "outputs": [],
      "source": [
        "demo_llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.0})\n",
        "\n",
        "def demo_qa_mistral_answer(riddle_content):\n",
        "  template = \"\"\" <s>[INST] You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must provide a short answer to a riddle. Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "      Here's an example to guide you:\n",
        "      Riddle: you might think i am a rather unstable character because i never stay at one place, however my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time, i can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules, in order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency,\n",
        "      Answer: oscillator\n",
        "\n",
        "      Read the riddle below and provide the correct answer.\n",
        "\n",
        "     Riddle: {riddle}\n",
        "\n",
        "      [/INST] </s>\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"riddle\"])\n",
        "  falcon_chain = LLMChain(prompt=prompt, llm=demo_llm)\n",
        "  answer = falcon_chain.run({\"riddle\":riddle_content})\n",
        "  return answer.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60fok56kRr6y"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "# chat mode instance\n",
        "chat = ChatOpenAI(temperature=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1zcsPVrRtu0"
      },
      "outputs": [],
      "source": [
        "# ChatGPT\n",
        "def live_qa_chatgpt_answer(riddle):\n",
        "  #template = \"\"\"\n",
        "  #  You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must first reason through the clues of the given riddle and then provide three short answers.\n",
        "  #  Your answers should be in a list of the form ['answer'1, 'answer2', 'answer3'].\n",
        "  #  Remember, each of your three answers should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "  #  Here is an example to guide you:\n",
        "  #  Riddle: you might think i am a rather unstable character because i never stay at one place, however my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time, i can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules, in order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency,\n",
        "  #  Answer: [oscilator, oscilator, oscilator]\n",
        "  #  The answers above are just an example, and are not the answer to the new riddle below. It is you job to figure out what the answer(s) to the new riddle is.\n",
        "  #  NOTE: YOUR ANSWER MUST STRICTLY BE A LIST OF THREE ANSWERS. You are allowed to include an answer multiple times if your reasoning shows that it is likely the correct answer, BUT UTLIMATELY, YOU MUST RETURN ONLY THREE ANSWERS IN TOTAL IN THE FORMAT: [Answer1, Answer2, Answer3].\n",
        "\n",
        "\n",
        "  #  Read the riddle below and provide the three possible correct answers in a list.\n",
        "\n",
        "  #  Riddle: {riddle}\n",
        "\n",
        "  #  Answer:\"\"\"\n",
        "\n",
        "  template = \"\"\"You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must first reason through the clues of the given riddle and then provide a short answer. Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "      Here's an example to guide you:\n",
        "      Riddle: You might think i am a rather unstable character because i never stay at one place. However my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time. I can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules. In order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency.\n",
        "      Answer: oscillator\n",
        "\n",
        "      Read the riddle below and provide the three possible correct answers as a json with keys: answer1, answer2, answer3\n",
        "\n",
        "      NOTE: You are allowed to include an answer multiple times if your reasoning shows that it is likely the correct answer. Do not provide any explanations.\n",
        "\n",
        "      Riddle: {riddle}\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  answer = chat([HumanMessage(content=template.format(riddle=riddle))])\n",
        "  return answer.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXaNho6NRvR7"
      },
      "outputs": [],
      "source": [
        "def demo_qa_chatgpt_answer(riddle_content):\n",
        "  template = \"\"\"\n",
        "      You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must provide a short answer to a riddle. Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "      Here's an example to guide you:\n",
        "      Riddle: you might think i am a rather unstable character because i never stay at one place, however my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time, i can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules, in order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency,\n",
        "      Answer: oscillator\n",
        "\n",
        "      Read the riddle below and provide the correct answer.\n",
        "      Riddle: {riddle}\n",
        "\n",
        "      Answer:\"\"\"\n",
        "\n",
        "  answer = chat([HumanMessage(content=template.format(riddle=riddle_content))])\n",
        "  return answer.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PCGVS0ORxIG"
      },
      "outputs": [],
      "source": [
        "import uvicorn\n",
        "import fastapi\n",
        "from pyngrok import ngrok\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjoOgTwQRxyt"
      },
      "outputs": [],
      "source": [
        "class DemoInputText(BaseModel):\n",
        "  text: str\n",
        "\n",
        "class LiveInputText(BaseModel):\n",
        "  clues: str\n",
        "  is_start_of_riddle: bool = False\n",
        "  is_end_of_riddle: bool = False\n",
        "  clue_count: int = 0\n",
        "\n",
        "class LiveDemoInputText(BaseModel):\n",
        "  clues: str\n",
        "  is_start_of_riddle: bool = False\n",
        "  is_end_of_riddle: bool = False\n",
        "  clue_count: int = 0\n",
        "  threshold = 4\n",
        "\n",
        "class OutputText(BaseModel):\n",
        "  mistral: str\n",
        "  chatGPT: str = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzDQuCnTijqF"
      },
      "source": [
        "# STT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypVGl-GF0vNl"
      },
      "outputs": [],
      "source": [
        "# Import and install the required libraries for asr\n",
        "\n",
        "%%capture\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer\n",
        "!pip install tabulate\n",
        "!pip install pydub\n",
        "!pip install transformers\n",
        "import torch\n",
        "import numpy as np\n",
        "import whisper\n",
        "import jiwer\n",
        "import time\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import joblib\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-qTvWkw6Lch"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install required libraries for web api\n",
        "!pip -q install fastapi\n",
        "!pip -q install pyngrok\n",
        "!pip -q install uvicorn\n",
        "!pip -q install nest_asyncio\n",
        "!pip -q install python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3VFaPh46Q61"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pyngrok import ngrok\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "import shutil\n",
        "\n",
        "# # # Import models for serialisation/ deserialisation\n",
        "from pydantic import BaseModel\n",
        "import base64\n",
        "import io\n",
        "import wave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKqpjy8N6Use"
      },
      "outputs": [],
      "source": [
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryL_OxBL6Xxf"
      },
      "outputs": [],
      "source": [
        "# Load whisper model\n",
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# DEVICE = \"cpu\"\n",
        "\n",
        "model = whisper.load_model(STT_WHISPER_MODEL, device = DEVICE) # Select whisper model size (tiny, base, small, medium, large)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA5Rq8XP6kLO"
      },
      "outputs": [],
      "source": [
        "def transcribe(path_to_audio):\n",
        "  \"\"\"Loads whisper model to transcribe audio\"\"\"\n",
        "\n",
        "  # Load audio\n",
        "  audio = whisper.load_audio(path_to_audio)\n",
        "\n",
        "  # Transcribe audio\n",
        "  result = model.transcribe(audio)\n",
        "\n",
        "  # Print transcript\n",
        "  return result[\"text\"]\n",
        "  # audio = whisper.load_audio(path_to_audio)\n",
        "  # audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "  # # Make log-Mel spectrogram and move to the same device as the model\n",
        "  # mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "  # # Decode the audio\n",
        "  # options = whisper.DecodingOptions(language= \"en\", without_timestamps= True, fp16 = False)\n",
        "  # result = whisper.decode(model, mel, options)\n",
        "\n",
        "  # return result.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH3Iy1f56of5"
      },
      "outputs": [],
      "source": [
        "def detect_start_point(transcribed_text):\n",
        "  \"\"\"Detects start points/ riddle cues present in audio transcripts\"\"\"\n",
        "\n",
        "  # Sample list of riddle start points\n",
        "  sample_start_points = [\"we begin\", \"i begin\", \"let's begin\",\\\n",
        "                         \"first riddle\", \"1st riddle\", \"riddle number one\", \"riddle number 1\",\\\n",
        "                         \"second riddle\", \"2nd riddle\", \"riddle number two\", \"riddle number 2\",\\\n",
        "                         \"third riddle\", \"3rd riddle\", \"riddle number three\", \"riddle number 3\",\\\n",
        "                         \"fourth riddle\", \"4th riddle\", \"riddle number four\", \"riddle number 4\",\\\n",
        "                         \"fifth riddle\", \"5th riddle\", \"riddle number five\", \"riddle number 5\",\\\n",
        "                         \"last riddle\", \"final riddle\", \"last one\", \"next one\", \"first one\", \\\n",
        "                         \"second one\", \"third one\", \"fourth one\", \"fifth one\",\\\n",
        "                         \"first redo\", \"second redo\", \"third redo\", \"fourth redo\", \"last redo\",\\\n",
        "                         \"final redo\", \"fifth redo\", \"best one\", \"fast riddle\", \"test riddle\"\n",
        "                         ]\n",
        "\n",
        "  # Check for a matching start point\n",
        "  matching_start_point = None\n",
        "  for start_point in sample_start_points:\n",
        "      if start_point in transcribed_text.lower():\n",
        "          matching_start_point = start_point\n",
        "          break\n",
        "\n",
        "  return matching_start_point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKUwoXdu6qJH"
      },
      "outputs": [],
      "source": [
        "def detect_end_point(transcribed_text):\n",
        "  \"\"\"Detects end points present in audio transcripts\"\"\"\n",
        "  #global matching_end_point\n",
        "\n",
        "  # Sample list of riddle start points\n",
        "  end_points = [\"who am i\"]\n",
        "\n",
        "  # Check for a matching start point\n",
        "  matching_end_point = None\n",
        "  for end_point in end_points:\n",
        "      if end_point in transcribed_text.lower():\n",
        "          matching_end_point = end_point\n",
        "          break\n",
        "\n",
        "  return matching_end_point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYq5c-Jb6tr9"
      },
      "outputs": [],
      "source": [
        "class BertClassifier(nn.Module):\n",
        "  def __init__(self, pretrained_bert, num_classes):\n",
        "    super(BertClassifier, self).__init__()\n",
        "    self.model = pretrained_bert\n",
        "    self.input_size = self.model.config.hidden_size\n",
        "    # Fully connected classifier\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(self.input_size, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, num_classes)\n",
        "    )\n",
        "  def forward(self, input_ids, attention_mask,labels=None):\n",
        "      outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      embedding = outputs.last_hidden_state.mean(dim=1)  #mean pooling\n",
        "      logits = self.classifier(embedding)\n",
        "\n",
        "      if labels is not None:\n",
        "          loss_fn=nn.CrossEntropyLoss()\n",
        "          loss=loss_fn(logits,labels)\n",
        "          return loss\n",
        "      return logits\n",
        "\n",
        "def preprocess_bert_features(sentence,tokenizer):\n",
        "  tokenized_input = tokenizer.encode_plus(sentence,padding='max_length',  max_length=512,truncation=True, return_tensors='pt', )\n",
        "  return tokenized_input[\"input_ids\"], tokenized_input[\"attention_mask\"]\n",
        "\n",
        "def predict_clue(sentence, model, tokenizer):\n",
        "    # Preprocess the sentence\n",
        "    input_ids, attention_mask = preprocess_bert_features(sentence, tokenizer)\n",
        "\n",
        "    # Ensure tensors are on the same device as the model and perform inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(STT_BERT_MODEL)\n",
        "pretrained_bert_model = BertModel.from_pretrained(STT_BERT_MODEL)\n",
        "bert_model = BertClassifier(pretrained_bert=pretrained_bert_model, num_classes=2)\n",
        "bert_model.load_state_dict(torch.load(STT_BERT_MODEL_DRIVE_LOCATION))\n",
        "#bert_model.load_state_dict(torch.load('/content/drive/MyDrive/Text classification/Models/bert_classifier_model.pth'))\n",
        "bert_model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIAJzi8J6xCK"
      },
      "outputs": [],
      "source": [
        "# Define a global variable to store the accumulated clues\n",
        "accumulated_clues = []\n",
        "store_to_count = []\n",
        "\n",
        "def process_audio_chunk(audio_filename):\n",
        "  \"\"\"Performs final piece audio transcription and riddle clue concatenation for the QA model\"\"\"\n",
        "\n",
        "  # Initialize variables\n",
        "  transcribed_text = \" \"  # To store the concatenated text\n",
        "  previous_end_index = 0  # To keep track of the end index of the previous riddle\n",
        "  clue_counter = 0 # count clues per new line\n",
        "  is_new_riddle = False\n",
        "  end_of_clues = False\n",
        "\n",
        "  # Transcribe audio chunk\n",
        "  chunk_transcript = transcribe(audio_filename)\n",
        "\n",
        "  # Detect start point\n",
        "  start_point = detect_start_point(chunk_transcript)\n",
        "\n",
        "  # Detect end point\n",
        "  end_point = detect_end_point(chunk_transcript)\n",
        "\n",
        "  # If a matching start point is found, concatenate text\n",
        "  if start_point:\n",
        "    is_new_riddle = True\n",
        "    accumulated_clues.clear()\n",
        "    store_to_count.clear()\n",
        "    start_index = chunk_transcript.lower().find(start_point.lower()) # identify first position of start-point phrase\n",
        "    previous_end_index = start_index + len(start_point) # set end position of start-point phrase\n",
        "\n",
        "  if end_point:\n",
        "    end_of_clues = True\n",
        "\n",
        "  # Add the transcribed chunk to the continuous text\n",
        "  transcribed_text = chunk_transcript[previous_end_index:].strip() # transcribed_text += chunk_transcript[previous_end_index:].strip()\n",
        "\n",
        "  # Process the text for riddle clues\n",
        "  sentences = re.split(r'(?<=[.,?])', transcribed_text)\n",
        "  # sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "  clues_found = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "    pred = predict_clue(sentence.strip(),bert_model,tokenizer)\n",
        "    if pred == 1:\n",
        "      accumulated_clues.append(sentence)  # Append the clue to the list\n",
        "      clues_found.append(sentence)\n",
        "\n",
        "  if clues_found:\n",
        "    grouped_clues = \" \".join(clues_found)\n",
        "    store_to_count.append(grouped_clues)\n",
        "    for i in range(len(store_to_count)):\n",
        "      clue_counter+=1\n",
        "    return chunk_transcript, \" \".join(accumulated_clues), clue_counter, is_new_riddle, end_of_clues #transcribed chunks, concatenated riddle clues, counter for clues, boolean if new riddle, boolean if riddle ends\n",
        "\n",
        "  return chunk_transcript, \" \", 0, is_new_riddle,  end_of_clues\n",
        "  clue_counter.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEMq2qChCRiP"
      },
      "source": [
        "# TTS API Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZxLPskWQC4t"
      },
      "outputs": [],
      "source": [
        "from starlette.middleware.gzip import GZipMiddleware\n",
        "app = FastAPI()\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "app.add_middleware(GZipMiddleware, minimum_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VP38yDKPRJM"
      },
      "outputs": [],
      "source": [
        "# @app.on_event(\"startup\")\n",
        "# def load_vits():\n",
        "# demo_config = VitsConfig()\n",
        "# live_config=VitsConfig()\n",
        "# demo_config.load_json(\"/content/drive/MyDrive/NSMQ AI Project/Technical/TTS/Prof Elsie Kauffmann/VITS model/vits-elsie/traineroutput/vits_vctk-May-24-2023_11+05PM-23a7a9a3/config.json\")\n",
        "# live_config.load_json(\"/content/drive/MyDrive/NSMQ AI Project/Technical/TTS/Isaac sesi/VITS model/sesi_vits/traineroutput/tts_multispeaker-October-13-2023_11+03AM-0000000/config.json\")\n",
        "# demo_vits = Vits.init_from_config(demo_config)\n",
        "# demo_vits.load_onnx(\"/content/drive/MyDrive/NSMQ AI Project/Technical/TTS/Prof Elsie Kauffmann/VITS model/vits-elsie/elsie.onnx\")\n",
        "# live_vits = Vits.init_from_config(live_config)\n",
        "# live_vits.load_onnx(\"/content/drive/MyDrive/NSMQ AI Project/Technical/TTS/Isaac sesi/VITS model/sesi.onnx\")\n",
        "\n",
        "demo_config = VitsConfig()\n",
        "demo_config.load_json(\"/path/to/onnx/config\")\n",
        "demo_vits = Vits.init_from_config(demo_config)\n",
        "demo_vits.load_onnx(\"path/to/onnx/model\")\n",
        "\n",
        "live_config=VitsConfig()\n",
        "live_config.load_json(\"path/to/onnx/config\")\n",
        "live_vits = Vits.init_from_config(live_config)\n",
        "live_vits.load_onnx(\"/path/to/onnx/model\")\n",
        "\n",
        "@app.get('/demo_tts')\n",
        "def onnx_audio(payload:DemoText):\n",
        "  out_path=demo_audio(payload.text,payload.voice)\n",
        "  return FileResponse(out_path, media_type=\"audio/wav\")\n",
        "\n",
        "@app.get('/live_tts')\n",
        "def onnx_audio(payload:LiveText):\n",
        "  out_path=live_audio(payload.text)\n",
        "  return FileResponse(out_path, media_type=\"audio/wav\")\n",
        "\n",
        "@app.get(\"/tts-test\", response_model=OutputTTSText)\n",
        "async def tts_test():\n",
        "    return {\"answer\": \"Hello from TTS\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjP9K6XCB2hm"
      },
      "source": [
        "# QA API CAll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMOctZiAO0ie"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "import random\n",
        "from glob import glob\n",
        "import time\n",
        "import string\n",
        "import re\n",
        "\n",
        "RIDDLE_ANSWERED_FILE_LIVE = os.path.join(QA_LOG_DRIVE_LOCATION, \"ans_live_logs.json\")\n",
        "with open(RIDDLE_ANSWERED_FILE_LIVE, 'w') as f:\n",
        "    json.dump({\"Mistral\": '', \"ChatGPT\": ''}, f)\n",
        "\n",
        "def remove_articles(text):\n",
        "    \"\"\"\n",
        "    Remove articles [the|a|an] from `text`\n",
        "\n",
        "    Args:\n",
        "        text: str\n",
        "\n",
        "    Returns:\n",
        "        text with articles removed: str\n",
        "    \"\"\"\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    return re.sub(regex, \" \", text)\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"\n",
        "    Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\n",
        "\n",
        "    Args:\n",
        "        s: (str) string to normalize\n",
        "\n",
        "    Returns:\n",
        "        normalized string: str\n",
        "    \"\"\"\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation.replace(\"/\", \"\"))\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(s)))\n",
        "\n",
        "\n",
        "\n",
        "def model_answer_confidence(model, threshold, chunk_num, model_answer, is_start_of_riddle):\n",
        "    cur_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")  # Get the current time\n",
        "\n",
        "    if is_start_of_riddle or chunk_num == 1:\n",
        "        # Create a new JSON log file if it is a new riddle.\n",
        "        print(f\"Is Start of Riddle: {is_start_of_riddle}\\t Clue Count: {chunk_num}\")\n",
        "        filename = os.path.join(QA_LOG_DRIVE_LOCATION, f\"{model}_log_{cur_time}.json\")\n",
        "        answer_counts = {}\n",
        "    else:\n",
        "        # Find the most recent generated JSON log file, if it is not a new riddle.\n",
        "        log_files = glob(os.path.join(QA_LOG_DRIVE_LOCATION, f\"{model}_log_*.json\"))\n",
        "        if log_files:\n",
        "            # Sort the log files by modification time to get the most recent one.\n",
        "            log_files.sort(key=os.path.getmtime, reverse=True)\n",
        "            filename = log_files[0]\n",
        "            with open(filename, 'r') as f:\n",
        "                #answer_counts = json.load(f)\n",
        "                logged_data = json.load(f)\n",
        "                answer_counts = logged_data[\"answer_counts\"]\n",
        "                print(\"Loaded Answer Counts Dictionary:\", answer_counts)\n",
        "        else:\n",
        "            # If no log files exist, create a new one.\n",
        "            filename = os.path.join(QA_LOG_DRIVE_LOCATION, f\"{model}_log_{cur_time}.json\")\n",
        "            answer_counts = {}\n",
        "\n",
        "    # Update answer_counts based on the model answer and chunk number\n",
        "    for ans in model_answer:\n",
        "        ans = remove_articles(normalize_text(ans).replace('\"', '')).strip()\n",
        "        answer_counts[ans] = answer_counts.get(ans, 0) + int(chunk_num)\n",
        "\n",
        "    answer_counts[''] = 0\n",
        "\n",
        "    print(\"Answer Counts Dictionary:\", answer_counts)\n",
        "\n",
        "    # Find the top answers and write answer_counts to the JSON log file\n",
        "    top_count = max(answer_counts.values())\n",
        "    top_answers = [ans for ans, count in answer_counts.items() if count == top_count]\n",
        "\n",
        "    top_answer = \"\"\n",
        "\n",
        "    if top_count >= threshold:\n",
        "        top_answer = random.choice(top_answers)\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        data_to_save = {\n",
        "            \"answer_counts\": answer_counts,\n",
        "            \"top_answer\": (top_answer, top_count)\n",
        "        }\n",
        "        #json.dump(answer_counts, f)\n",
        "        json.dump(data_to_save, f)\n",
        "\n",
        "        print(\"Saved Data:\", data_to_save)\n",
        "\n",
        "    return top_answer, top_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK9UVRz-14bD"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import json\n",
        "\n",
        "\n",
        "def preprocess_model_output(model_output):\n",
        "    if isinstance(model_output, dict):\n",
        "        return model_output\n",
        "\n",
        "    # Convert model output to string\n",
        "    model_output = str(model_output).replace(\"\\n\", '').strip()\n",
        "\n",
        "    pattern = r'{.*?}'\n",
        "    m = re.search(pattern, model_output)\n",
        "    model_output = m.group(0)\n",
        "\n",
        "    # Remove ` characters if any\n",
        "    model_output = model_output.replace('```', '').replace('json', '')\n",
        "\n",
        "    # Answer not surrounded in curly braces\n",
        "    if not model_output.startswith(\"{\") or not model_output.endswith(\"}\"):\n",
        "        model_output = '{' + model_output + '}'\n",
        "\n",
        "    # Replace null in quotes and replace with none\n",
        "    model_output = model_output.replace(\": null\", \"'null'\")\n",
        "    print(\"State of Model Output:\", model_output)\n",
        "\n",
        "    # Try converting answer to json\n",
        "    try:\n",
        "        #model_output_dict = ast.literal_eval(model_output)\n",
        "        json_data = json.loads(model_output)\n",
        "        return json_data\n",
        "    except (SyntaxError, ValueError):\n",
        "        print(\"SOMETHING WENT WRONG!\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2WvaAA5Oh5O"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def filter_answers(ans_data, confidence_threshold, is_end_of_riddle):\n",
        "    confidence = ans_data[1]\n",
        "    if int(confidence) >= confidence_threshold or is_end_of_riddle:\n",
        "      return ans_data[0]\n",
        "\n",
        "    return ''\n",
        "\n",
        "\n",
        "def load_riddle_answered_log(is_start_of_riddle=True, chunk_num=1):\n",
        "    if is_start_of_riddle == True:\n",
        "      return {\"mistral\": '' , \"chatGPT\": ''}\n",
        "    else:\n",
        "      if os.path.exists(RIDDLE_ANSWERED_FILE_LIVE):\n",
        "        with open(RIDDLE_ANSWERED_FILE_LIVE, \"r\") as file:\n",
        "          return json.load(file)\n",
        "      else:\n",
        "        return {\"mistral\": '', \"chatGPT\": ''}\n",
        "\n",
        "def save_riddle_answered_log(data):\n",
        "    with open(RIDDLE_ANSWERED_FILE_LIVE, \"w\") as file:\n",
        "        json.dump(data, file)\n",
        "\n",
        "\n",
        "@app.get(\"/live_qa\", response_model=OutputText)\n",
        "def live_answer(input_data: LiveInputText):\n",
        "    ct = 10.0  # ct represents the confidence threshold.\n",
        "    chunk_num = input_data.clue_count\n",
        "    is_start_of_riddle = input_data.is_start_of_riddle\n",
        "    is_end_of_riddle = input_data.is_end_of_riddle\n",
        "\n",
        "    print()\n",
        "    print(\"Chunk Num:\", chunk_num)\n",
        "    print(\"=\"*70)\n",
        "    print(input_data.clues)\n",
        "    print()\n",
        "\n",
        "    # Load data previously computed answers\n",
        "    ans_file_st = time.time()\n",
        "    answer_file = load_riddle_answered_log(is_start_of_riddle, chunk_num)\n",
        "    ans_file_et = time.time()\n",
        "    print(\"Time to load answers file:\", (ans_file_et-ans_file_st))\n",
        "\n",
        "    if answer_file['mistral'] == '' and chunk_num != 0:\n",
        "        # Send clues to falcon and get answer\n",
        "        mistral_start_time = time.time()\n",
        "        mistral_output = answer_with_mistral(riddle=input_data.clues)\n",
        "        mistral_end_time = time.time()\n",
        "        print(\"\\nMistral Inference Time:\", mistral_end_time-mistral_start_time)\n",
        "        #print(f\"Mistral Output: {mistral_output}\")\n",
        "        p_st = time.time()\n",
        "        mistral_output = preprocess_model_output(mistral_output)\n",
        "        p_et = time.time()\n",
        "        print(\"Time to process Mistral Output:\", p_et - p_st)\n",
        "\n",
        "        #Put answers in a list\n",
        "        if mistral_output is not None:\n",
        "            mistral_output = [mistral_output[key] for key in mistral_output.keys()]\n",
        "        else:\n",
        "          mistral_output = ['']\n",
        "          print(\"Failed to convert Mistral response to dict/json\")\n",
        "        mistral_conf_st = time.time()\n",
        "        mistral_ans_data = model_answer_confidence(\"Mistral\", ct, chunk_num, mistral_output, is_start_of_riddle)\n",
        "        mistral_conf_et = time.time()\n",
        "        print(\"\\nMistral Confidence Modelling Time:\", mistral_conf_et-mistral_conf_st)\n",
        "        filter_start_time = time.time()\n",
        "        mistral_final_ans = filter_answers(mistral_ans_data, ct, is_end_of_riddle)\n",
        "        filter_end_time = time.time()\n",
        "        print(\"\\nMistral Time Elapsed for Filtering:\", (filter_end_time-filter_start_time))\n",
        "        #print(mistral_ans_data)\n",
        "    else:\n",
        "      mistral_final_ans = answer_file['mistral']\n",
        "\n",
        "    if answer_file['chatGPT'] == '' and chunk_num != 0:\n",
        "        # Send clues to ChatGPT and get answer\n",
        "        chatgpt_start_time = time.time()\n",
        "        #chatGPT_output = live_qa_chatgpt_answer(input_data.clues)\n",
        "        chatGPT_output = {\"answer1\": ''}\n",
        "        chatgpt_end_time = time.time()\n",
        "        print(\"\\nChatGPT Inference Time:\", chatgpt_end_time-chatgpt_start_time)\n",
        "        #print(f\"ChatGPT Output: {chatGPT_output}\")\n",
        "        cp_st = time.time()\n",
        "        chatGPT_output = preprocess_model_output(chatGPT_output)\n",
        "        cp_et = time.time()\n",
        "        print(\"Time to preprocess ChatGPT output:\", (cp_et - cp_st))\n",
        "        # Put answers in a list\n",
        "        if chatGPT_output is not None:\n",
        "            chatGPT_output = [chatGPT_output[key] for key in chatGPT_output.keys()]\n",
        "        else:\n",
        "          chatGPT_output = ['',]\n",
        "          print(\"Failed to convert ChatGPT response to dict/json\")\n",
        "        c_conf_st = time.time()\n",
        "        chatGPT_ans_data = model_answer_confidence(\"ChatGPT\", ct, chunk_num, chatGPT_output, is_start_of_riddle)\n",
        "        c_conf_et = time.time()\n",
        "        print(\"\\nChatGPT Confidence Modelling Time:\", c_conf_et-c_conf_st)\n",
        "        filter_start_time = time.time()\n",
        "        chatgpt_final_ans = filter_answers(chatGPT_ans_data, ct, is_end_of_riddle)\n",
        "        filter_end_time = time.time()\n",
        "        print(\"\\nChatGPT Time Elapsed for Filtering:\", (filter_end_time-filter_start_time))\n",
        "        #print(chatGPT_ans_data)\n",
        "    else:\n",
        "      chatgpt_final_ans = answer_file['chatGPT']\n",
        "\n",
        "\n",
        "    answers = {\n",
        "        \"mistral\": mistral_final_ans,\n",
        "        \"chatGPT\": chatgpt_final_ans\n",
        "    }\n",
        "\n",
        "    # Write answers to file\n",
        "    save_st = time.time()\n",
        "    save_riddle_answered_log(answers)\n",
        "    save_et = time.time()\n",
        "    print(\"\\nTime to save file:\", (save_et-save_st))\n",
        "\n",
        "    return answers\n",
        "\n",
        "\n",
        "@app.get(\"/live_demo_qa\", response_model=OutputText)\n",
        "def live_demo_answer(input_data: LiveDemoInputText):\n",
        "    ct = 3  # ct represents the confidence threshold.\n",
        "    chunk_num = input_data.clue_count\n",
        "    is_start_of_riddle = input_data.is_start_of_riddle\n",
        "    is_end_of_riddle = input_data.is_end_of_riddle\n",
        "\n",
        "    print()\n",
        "    print(\"Chunk Num:\", chunk_num)\n",
        "    print(\"=\"*70)\n",
        "    print(input_data.clues)\n",
        "    print()\n",
        "\n",
        "    # Load data previously computed answers\n",
        "    ans_file_st = time.time()\n",
        "    answer_file = load_riddle_answered_log(is_start_of_riddle, chunk_num)\n",
        "    ans_file_et = time.time()\n",
        "    print(\"Time to load answers file:\", (ans_file_et-ans_file_st))\n",
        "\n",
        "    if answer_file['mistral'] == '' and chunk_num != 0:\n",
        "        # Send clues to falcon and get answer\n",
        "        mistral_start_time = time.time()\n",
        "        mistral_output = answer_with_mistral(riddle=input_data.clues)\n",
        "        mistral_end_time = time.time()\n",
        "        print(\"\\nMistral Inference Time:\", mistral_end_time-mistral_start_time)\n",
        "        #print(f\"Mistral Output: {mistral_output}\")\n",
        "        p_st = time.time()\n",
        "        mistral_output = preprocess_model_output(mistral_output)\n",
        "        p_et = time.time()\n",
        "        print(\"Time to process Mistral Output:\", p_et - p_st)\n",
        "\n",
        "        #Put answers in a list\n",
        "        if mistral_output is not None:\n",
        "            mistral_output = [mistral_output[key] for key in mistral_output.keys()]\n",
        "        else:\n",
        "          mistral_output = ['']\n",
        "          print(\"Failed to convert Mistral response to dict/json\")\n",
        "        mistral_conf_st = time.time()\n",
        "        mistral_ans_data = model_answer_confidence(\"Mistral\", ct, chunk_num, mistral_output, is_start_of_riddle)\n",
        "        mistral_conf_et = time.time()\n",
        "        print(\"\\nMistral Confidence Modelling Time:\", mistral_conf_et-mistral_conf_st)\n",
        "        filter_start_time = time.time()\n",
        "        mistral_final_ans = filter_answers(mistral_ans_data, ct, is_end_of_riddle)\n",
        "        filter_end_time = time.time()\n",
        "        print(\"\\nMistral Time Elapsed for Filtering:\", (filter_end_time-filter_start_time))\n",
        "        #print(mistral_ans_data)\n",
        "    else:\n",
        "      mistral_final_ans = answer_file['mistral']\n",
        "\n",
        "    if answer_file['chatGPT'] == '' and chunk_num != 0:\n",
        "        # Send clues to ChatGPT and get answer\n",
        "        chatgpt_start_time = time.time()\n",
        "        #chatGPT_output = live_qa_chatgpt_answer(input_data.clues)\n",
        "        chatGPT_output = {\"answer1\": ''}\n",
        "        chatgpt_end_time = time.time()\n",
        "        print(\"\\nChatGPT Inference Time:\", chatgpt_end_time-chatgpt_start_time)\n",
        "        #print(f\"ChatGPT Output: {chatGPT_output}\")\n",
        "        cp_st = time.time()\n",
        "        chatGPT_output = preprocess_model_output(chatGPT_output)\n",
        "        cp_et = time.time()\n",
        "        print(\"Time to preprocess ChatGPT output:\", (cp_et - cp_st))\n",
        "        # Put answers in a list\n",
        "        if chatGPT_output is not None:\n",
        "            chatGPT_output = [chatGPT_output[key] for key in chatGPT_output.keys()]\n",
        "        else:\n",
        "          chatGPT_output = ['',]\n",
        "          print(\"Failed to convert ChatGPT response to dict/json\")\n",
        "        c_conf_st = time.time()\n",
        "        chatGPT_ans_data = model_answer_confidence(\"ChatGPT\", ct, chunk_num, chatGPT_output, is_start_of_riddle)\n",
        "        c_conf_et = time.time()\n",
        "        print(\"\\nChatGPT Confidence Modelling Time:\", c_conf_et-c_conf_st)\n",
        "        filter_start_time = time.time()\n",
        "        chatgpt_final_ans = filter_answers(chatGPT_ans_data, ct, is_end_of_riddle)\n",
        "        filter_end_time = time.time()\n",
        "        print(\"\\nChatGPT Time Elapsed for Filtering:\", (filter_end_time-filter_start_time))\n",
        "        #print(chatGPT_ans_data)\n",
        "    else:\n",
        "      chatgpt_final_ans = answer_file['chatGPT']\n",
        "\n",
        "\n",
        "    answers = {\n",
        "        \"mistral\": mistral_final_ans,\n",
        "        \"chatGPT\": chatgpt_final_ans\n",
        "    }\n",
        "\n",
        "    # Write answers to file\n",
        "    save_st = time.time()\n",
        "    save_riddle_answered_log(answers)\n",
        "    save_et = time.time()\n",
        "    print(\"\\nTime to save file:\", (save_et-save_st))\n",
        "\n",
        "    return answers\n",
        "\n",
        "\n",
        "@app.get('/demo_qa', response_model=OutputText)\n",
        "def demo_answer(input_data: DemoInputText):\n",
        "    print(\"Demo mode\")\n",
        "    print(input_data)\n",
        "    riddle_content = input_data.text\n",
        "    print(riddle_content)\n",
        "    falcon_ans = demo_qa_mistral_answer(riddle_content)\n",
        "\n",
        "    print(\"Mistral:\", falcon_ans)\n",
        "\n",
        "    answers = {\n",
        "        \"mistral\": falcon_ans\n",
        "    }\n",
        "    return answers\n",
        "\n",
        "\n",
        "@app.get(\"/qa-test\", response_model=OutputText)\n",
        "async def qa_test():\n",
        "    return {\n",
        "        \"mistral\": \"Mistral Says Hello!\",\n",
        "        \"chatGPT\": \"ChatGPT Says Hello!\"\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyEJZexkBKvS"
      },
      "source": [
        "#STT API CALL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUzQx_5t62GR"
      },
      "outputs": [],
      "source": [
        "class AudioBytes(BaseModel):\n",
        "  data: bytes\n",
        "  filename: str\n",
        "\n",
        "@app.get(\"/get-transcript\")\n",
        "async def get_transcript(audio: AudioBytes):\n",
        "  try:\n",
        "    decoded_data = base64.b64decode(audio.data)\n",
        "\n",
        "    # Write bytes data to a .wav file\n",
        "    with io.BytesIO(decoded_data) as audio_file:\n",
        "        with wave.open(audio_file, \"wb\") as wav:\n",
        "          wav.setnchannels(1)\n",
        "          wav.setsampwidth(2)\n",
        "          wav.setframerate(16000)\n",
        "\n",
        "          # Write .wav files\n",
        "          wav.writeframes(decoded_data)\n",
        "\n",
        "    # Save the audio file with the custom name\n",
        "    audio_filename = audio.filename\n",
        "    with open(audio_filename, \"wb\") as file:\n",
        "        file.write(decoded_data)\n",
        "\n",
        "    # Get transcript and delete temporary audio file\n",
        "    chunk_transcript, current_clues, clue_counter, is_new_riddle, end_of_clues = process_audio_chunk(audio_filename) # current clues contains previous+recently identified clues\n",
        "    os.remove(audio_filename)\n",
        "    print(\"transcript:\", chunk_transcript)\n",
        "    return {\"transcript\": chunk_transcript, \"clues\": current_clues, \"clue_count\":clue_counter, \"is_start_of_riddle\":is_new_riddle, \"is_end_of_riddle\":end_of_clues}\n",
        "  except Exception as e:\n",
        "    return {\"error\":str(e)}\n",
        "\n",
        "@app.get(\"/stt-test\")\n",
        "async def stt_test():\n",
        "  return {\"transcript\":\"Hello from STT.\", \"clues\":\"\", \"clue_count\":\"\", \"is_start_of_riddle\":\"\", \"is_end_of_riddle\":\"\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oYS7TlYeRXW"
      },
      "source": [
        "# Setup required API Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-u3mcP7P4b-"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn pydantic pyngrok nest_asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6D-5UHXxLGw"
      },
      "source": [
        "# GENERIC API CALL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JljlQd_HX-x1"
      },
      "outputs": [],
      "source": [
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"response\": \"Hello from NSMQ AI\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd6hsY1wGvuH"
      },
      "source": [
        "# API SERVER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2IEFh8mFDuV"
      },
      "source": [
        "Run the cells below to set up the API server and public URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhrSV7f5eCFa"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken NGROK_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye8s6_uNeDVV"
      },
      "outputs": [],
      "source": [
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkvdCODTR2fg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}