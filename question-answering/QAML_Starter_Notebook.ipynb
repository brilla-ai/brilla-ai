{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AfricaAIED 2024 Hackathon\n",
        "AfricaAIED 2024 Hackathon üöÄ\n",
        "Welcome to the AfricaAIED 2024 Hackathon! This notebook is designed to help you get started with answering scientific riddles from the National Science and Maths Quiz (NSMQ) using the Mistral-7B-Instruct model. The setup is similar to what we used to compete with students during the finals of the 2023 NSMQ, excluding the confidence modeling pipeline.\n",
        "\n",
        "You are provided with a sample riddle to experiment with, but feel free to experiment with riddles of your own. ü§îüß†\n",
        "\n",
        "Happy Building, and best of luck in the Hackathon. We can't wait to see what you build. üéâüèÜ\n",
        "\n",
        "P.S. This notebook will run fine on a T4 GPU, albeit slowly. For optimal performance, consider using an A100."
      ],
      "metadata": {
        "id": "pr0wFRITYClN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6YX-vQvIX9ZM"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers\n",
        "!pip install -q accelerate\n",
        "!pip install -q einops\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain-community\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To use the Mistral-7B-Instruct-v0.1 model from Mistral-AI,\n",
        "# 1. Request for access to the model from: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
        "# 2. Create an Access Token from your HuggingFace account, if you don't have one already.\n",
        "# 3. Add the Access Token to your colab secrets under the name \"HF_TOKEN\"\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "PeJtz3RP6qkS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "pvFlOgGIbRvj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch Mistral-7B-Instruct-v0.1 model\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "\n",
        "pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        use_cache=True,\n",
        "        device_map=\"auto\",\n",
        "        max_length=512,\n",
        "        temperature = 0.7,\n",
        "        do_sample=False,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "bl1mPGGcbW4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = template = \"\"\"  <s>[INST]   You are a science prodigy currently competing in a National Science competition. \\\n",
        "        You are now in the fifth round, where you must provide a one-word answer to a riddle. \\\n",
        "        Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. \\\n",
        "        Adding additional text will result in point deductions.\n",
        "        Here's an example to guide you: \\\n",
        "        Riddle: You might think I am a rather unstable character because I never stay at one place. \\\n",
        "                However my motion obeys strict rules and I always return to where I started and even if I have to leave that spot again I do it in strict accordance to time. \\\n",
        "                I can be named in electrical and mechanical contexts. \\\n",
        "                In all cases i obey the same mathematical rules. \\\n",
        "                In order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency. \\\n",
        "        Answer: oscillator\n",
        "\n",
        "        Read the riddle below and provide the correct answer.\n",
        "\n",
        "        Riddle: {riddle}\n",
        "        Answer:\n",
        "        [/INST] </s>\n",
        "     \"\"\"\n",
        "def answer_with_mistral(riddle):\n",
        "    prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=[\"riddle\"])\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "    answer = llm_chain.run({\"riddle\":riddle})\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "R4-fbTZ2bb74"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_riddle = \"\"\"\n",
        "    I am a set of points.\n",
        "    I am the set of all possible positions a point can take.\n",
        "    I may be a region in a plane or in space, but more often I am a continuous curve\n",
        "    I am the path followed by a point subject to some constraint. Who am I?\n",
        "\"\"\"\n",
        "\n",
        "# Generate an answer for sample riddle assuming all clues have been read.\n",
        "# P.S. This will take some time to run if you are running on a T4 GPU.\n",
        "generated_answer = answer_with_mistral(riddle=sample_riddle)\n",
        "print(generated_answer)"
      ],
      "metadata": {
        "id": "WcxYftEHcweA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1rIvBtPaYFOf"
      }
    }
  ]
}