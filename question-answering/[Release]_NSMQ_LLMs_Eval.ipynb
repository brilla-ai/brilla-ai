{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzDQuCnTijqF"
      },
      "source": [
        "Install required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypeXsY_pjtGI"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate einops langchain bitsandbytes xformers\n",
        "!pip install -q python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfrFzIYkXEkp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVhMqdXCFZP_"
      },
      "outputs": [],
      "source": [
        "# Fetch train and test splis of dataset from directory\n",
        "# Replace the value of 'DATA_DIR' with the path to data directory\n",
        "DATA_DIR = \"path/to/dataset\"\n",
        "ds_split_map = {\n",
        "    \"train\":os.path.join(DATA_DIR, \"Kwame AI - NSMQ Riddles Train\"),\n",
        "    \"test\":os.path.join(DATA_DIR, \"Kwame AI - NSMQ Riddles Test\"),\n",
        "    \"test_2019\":os.path.join(DATA_DIR, \"NSMQ Contest with Video Links - 2019\"),\n",
        "    \"test_2020\":os.path.join(DATA_DIR, \"NSMQ Contest with Video Links - 2020\"),\n",
        "    \"test_2019_with_contexts\":os.path.join(DATA_DIR, \"NSMQ Contest with Video Links - 2019_with_contexts\"),\n",
        "    \"test_2020_with_contexts\":os.path.join(DATA_DIR, \"NSMQ Contest with Video Links - 2020_with_contexts\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYfeJgD_j96r"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXX2hm9wh2TE"
      },
      "outputs": [],
      "source": [
        "# Fetch Mistral-7B-Instruct-v0.1 model\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "model_name = model_id.split('/')[-1]\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "\n",
        "pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        use_cache=True,\n",
        "        device_map=\"auto\",\n",
        "        max_length=500,\n",
        "        temperature = 1,\n",
        "        do_sample=False,\n",
        "        #top_k=1,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        torch_dtype=torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6KzsIXEkAfP"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Function to get mistral answer for live riddles\n",
        "def answer_with_mistral(riddle):\n",
        "  template = \"\"\"  <s>[INST]\n",
        "      You are a science prodigy currently competing in a National Science competition. \\\n",
        "      You are now in the fifth round, where you must provide a one-word answer to a riddle. \\\n",
        "      Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. \\\n",
        "      Adding additional text will result in point deductions. \\\n",
        "      Here's an example to guide you:\n",
        "      Riddle: You might think I am a rather unstable character because I never stay at one place. \\\n",
        "            However, my motion obeys strict rules and I always return to where I started \\\n",
        "            And even if i have to leave that spot again I do it in strict accordance to time. \\\n",
        "            I can be named in electrical and mechanical contexts \\\n",
        "            In all cases, I obey the same mathematical rules. \\\n",
        "            In order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency. \\\n",
        "\n",
        "      Answer: oscillator\n",
        "\n",
        "      Read the riddle below and provide the correct answer.\n",
        "\n",
        "      Riddle: {riddle}\n",
        "      Answer:\n",
        "      [/INST] </s>\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"riddle\"])\n",
        "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "  answer = llm_chain.run({\"riddle\":riddle})\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSpHVwQDnecx"
      },
      "outputs": [],
      "source": [
        "import string, re\n",
        "\n",
        "\n",
        "def remove_articles(text):\n",
        "    \"\"\"\n",
        "      Remove articles [the|a|an] from `text`\n",
        "\n",
        "      Args:\n",
        "        text: str\n",
        "\n",
        "      Returns:\n",
        "        text with articles removed: str\n",
        "    \"\"\"\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    return re.sub(regex, \" \", text)\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"\n",
        "      Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\n",
        "\n",
        "      Args:\n",
        "        s: (str) string to normalize\n",
        "\n",
        "      Returns:\n",
        "        noralized string: str\n",
        "    \"\"\"\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation.replace(\"/\", \"\"))\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(s)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIu3i6hunIm0"
      },
      "outputs": [],
      "source": [
        "import Levenshtein\n",
        "\n",
        "def per_clue_based_fm(model, fm_scores, answers, model_answer, threshold=0.70):\n",
        "    \"\"\"\n",
        "    Computes Fuzzy Match Score\n",
        "    Fuzzy Match: percentage of times model_answer is close to ground_truth\n",
        "\n",
        "    Args:\n",
        "      model: str - model being used\n",
        "      answers: list[str] - ground truth answers to riddle\n",
        "      model_answer: str output of model\n",
        "      threshold: float - similarity threshold (e.g., 0.8 for 80% similarity)\n",
        "\n",
        "    Returns:\n",
        "      bool: True/False\n",
        "    \"\"\"\n",
        "    # Remove empty entries from answers\n",
        "    answers = [answer for answer in answers if answer.strip() != '_' or answer.strip() != '']\n",
        "\n",
        "    if any(ground_truth in model_answer for ground_truth in answers) or any(model_answer in ground_truth for ground_truth in answers):\n",
        "      fm_scores[model] += 1\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def per_clue_based_em(model, em_scores, answers, model_answer):\n",
        "  \"\"\"\n",
        "    Compute Exact Match Score.\n",
        "    Exact Match: Number of times model_answer exactly equals ground_truth\n",
        "\n",
        "    Args:\n",
        "      model: str - model being used\n",
        "      answers: list[str] - ground truth answers to riddle\n",
        "      model_answer: str output of model\n",
        "\n",
        "    Returns:\n",
        "      bool: True/False\n",
        "  \"\"\"\n",
        "  # Remove emptry entries from answers\n",
        "  answers = [answer for answer in answers if answer.strip() != '_' or answer.strip() != '']\n",
        "  if any(ground_truth == model_answer for ground_truth in answers):\n",
        "    em_scores[model] += 1\n",
        "    return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5URp681Tp5e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_dataset(filepath: str):\n",
        "    # Read the entire dataset from the CSV file\n",
        "    df = pd.read_csv(f\"{filepath}.csv\")\n",
        "\n",
        "    # Split the data by year into two DataFrames\n",
        "    df_2019 = df[df['Year'] == 2019]\n",
        "    df_2020 = df[df['Year'] == 2020]\n",
        "\n",
        "    # Fetch clues, answers, and subjects from the 2019 DataFrame and convert to lists\n",
        "    clues_2019 = df_2019[[\"Clue 1\", \"Clue 2\", \"Clue 3\", \"Clue 4\", \"Clue 5\", \"Clue 6\", \"Clue 7\", \"Clue 8\"]]\n",
        "    clues_2019.fillna(\"\", inplace=True)\n",
        "    clues_2019 = clues_2019.values.tolist()\n",
        "\n",
        "    answers_2019 = df_2019[[\"Answer 1\", \"Answer 2\", \"Answer 3\", \"Answer 4\"]]\n",
        "    answers_2019.fillna(\"_\", inplace=True)\n",
        "    answers_2019 = answers_2019.values.tolist()\n",
        "\n",
        "    subjects_2019 = df_2019[\"Subject\"].tolist()\n",
        "\n",
        "    # Fetch clues, answers, and subjects from the 2020 DataFrame and convert to lists\n",
        "    clues_2020 = df_2020[[\"Clue 1\", \"Clue 2\", \"Clue 3\", \"Clue 4\", \"Clue 5\", \"Clue 6\", \"Clue 7\", \"Clue 8\"]]\n",
        "    clues_2020.fillna(\"\", inplace=True)\n",
        "    clues_2020 = clues_2020.values.tolist()\n",
        "\n",
        "    answers_2020 = df_2020[[\"Answer 1\", \"Answer 2\", \"Answer 3\", \"Answer 4\"]]\n",
        "    answers_2020.fillna(\"_\", inplace=True)\n",
        "    answers_2020 = answers_2020.values.tolist()\n",
        "\n",
        "    subjects_2020 = df_2020[\"Subject\"].tolist()\n",
        "\n",
        "    return (clues_2019, answers_2019, subjects_2019), (clues_2020, answers_2020, subjects_2020)\n",
        "\n",
        "\n",
        "filepath = ds_split_map[\"test\"]\n",
        "eval_dataset_2019, eval_dataset_2020 = read_dataset(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y28usVy0vvOX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def human_eval(eval_dataset, year):\n",
        "    # Initialize EM and FM dictionaries\n",
        "    fm_scores = {\n",
        "        model_name: 0\n",
        "    }\n",
        "\n",
        "    em_scores = {\n",
        "        model_name: 0\n",
        "    }\n",
        "\n",
        "    riddles, ground_truth_answers, subjects = eval_dataset\n",
        "\n",
        "    # Create dictionaries to store EM and FM scores per subject\n",
        "    em_scores_per_subject = {}\n",
        "    fm_scores_per_subject = {}\n",
        "\n",
        "    # Get unique subjects\n",
        "    unique_subjects = set(subjects)\n",
        "\n",
        "    # Initialize scores for each subject\n",
        "    for subject in unique_subjects:\n",
        "        em_scores_per_subject[subject] = {model_name: 0}\n",
        "        fm_scores_per_subject[subject] = {model_name: 0}\n",
        "\n",
        "    # Create a list to store log data for each riddle\n",
        "    log_data = []\n",
        "\n",
        "    # Iterate over riddles\n",
        "    for riddle_num, (riddle, answer, subject) in enumerate(zip(riddles, ground_truth_answers, subjects), start=1):\n",
        "        # Initialize EM and FM values\n",
        "        em_value = False\n",
        "        fm_value = False\n",
        "\n",
        "        clues = riddle\n",
        "        clues = [clue for clue in clues if clue.strip() != '']\n",
        "        answers = [remove_articles(normalize_text(ans)).strip() for ans in answer]\n",
        "        answers = [ans if ans != '' else '_' for ans in answers]\n",
        "\n",
        "        cur_clues = ''\n",
        "        fm_answer_found = False\n",
        "\n",
        "        for clue_num, clue in enumerate(clues, start=1):\n",
        "            cur_clues += '\\n ' + clue\n",
        "\n",
        "            model_answer = answer_with_mistral(cur_clues).lower().replace(\"answer to the riddle is\", '')\n",
        "            model_answer = remove_articles(normalize_text(model_answer.split(':')[-1]).replace('\"', '')).strip()\n",
        "\n",
        "            em_check = per_clue_based_em(model_name, em_scores, answers, model_answer)\n",
        "\n",
        "            if not fm_answer_found:\n",
        "                fm_check = per_clue_based_fm(model_name, fm_scores, answers, model_answer)\n",
        "                if fm_check:\n",
        "                    fm_answer_found = True\n",
        "\n",
        "            if em_check:\n",
        "                em_value = True\n",
        "                fm_value = True\n",
        "                break\n",
        "\n",
        "            if fm_check:\n",
        "                fm_value = True\n",
        "\n",
        "        # Create a dictionary for the log data of the current riddle\n",
        "        log_entry = {\n",
        "            \"Clue 1\": riddle[0] if len(riddle) > 0 else \"\",\n",
        "            \"Clue 2\": riddle[1] if len(riddle) > 1 else \"\",\n",
        "            \"Clue 3\": riddle[2] if len(riddle) > 2 else \"\",\n",
        "            \"Clue 4\": riddle[3] if len(riddle) > 3 else \"\",\n",
        "            \"Clue 5\": riddle[4] if len(riddle) > 4 else \"\",\n",
        "            \"Clue 6\": riddle[5] if len(riddle) > 5 else \"\",\n",
        "            \"Clue 7\": riddle[6] if len(riddle) > 6 else \"\",\n",
        "            \"Clue 8\": riddle[7] if len(riddle) > 7 else \"\",\n",
        "            \"Answer 1\": answer[0] if len(answer) > 0 else \"\",\n",
        "            \"Answer 2\": answer[1] if len(answer) > 1 else \"\",\n",
        "            \"Answer 3\": answer[2] if len(answer) > 2 else \"\",\n",
        "            \"Answer 4\": answer[3] if len(answer) > 3 else \"\",\n",
        "            \"Model Answer\": model_answer,\n",
        "            \"EM Value\": em_value,\n",
        "            \"FM Value\": fm_value,\n",
        "            \"Riddle Answered On\": clue_num,\n",
        "            \"Subject\": subject\n",
        "        }\n",
        "\n",
        "        # Append the log data to the list\n",
        "        log_data.append(log_entry)\n",
        "\n",
        "        # Update EM and FM scores per subject\n",
        "        em_check_per_subject = em_scores_per_subject[subject].get(model_name, 0)\n",
        "        em_scores_per_subject[subject][model_name] = em_check_per_subject + (1 if em_value else 0)\n",
        "\n",
        "        fm_check_per_subject = fm_scores_per_subject[subject].get(model_name, 0)\n",
        "        fm_scores_per_subject[subject][model_name] = fm_check_per_subject + (1 if fm_value else 0)\n",
        "\n",
        "        # Print a message indicating the completion of logging for the current riddle\n",
        "        print(f\"Logged riddle {riddle_num}\")\n",
        "\n",
        "    # Create a DataFrame from the log data\n",
        "    log_df = pd.DataFrame(log_data)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    log_filename = os.path.join(DATA_DIR, f\"Results/{model_name}_{year}_human_eval_log.csv\")\n",
        "    log_df.to_csv(log_filename, index=False)\n",
        "\n",
        "    return em_scores, em_scores_per_subject, fm_scores, fm_scores_per_subject\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rj89nU3oFBc"
      },
      "outputs": [],
      "source": [
        "# Get scores for 2019 riddles\n",
        "scores_2019 = human_eval(eval_dataset_2019, 2019)\n",
        "\n",
        "# Get scores for 2020 riddles\n",
        "scores_2020 = human_eval(eval_dataset_2020, 2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STwqdweZf1gg"
      },
      "outputs": [],
      "source": [
        "scores_2019, scores_2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65shwSvi-aKQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_subject_counts(filepath: str):\n",
        "    # Read the entire dataset from the CSV file\n",
        "    df = pd.read_csv(f\"{filepath}.csv\")\n",
        "\n",
        "    # Split the data by year into two DataFrames\n",
        "    df_2019 = df[df['Year'] == 2019]\n",
        "    df_2020 = df[df['Year'] == 2020]\n",
        "\n",
        "    # Fetch subjects from the 2019 DataFrame and count their occurrences\n",
        "    subjects_2019_counts = df_2019[\"Subject\"].value_counts().to_dict()\n",
        "\n",
        "    # Fetch subjects from the 2020 DataFrame and count their occurrences\n",
        "    subjects_2020_counts = df_2020[\"Subject\"].value_counts().to_dict()\n",
        "\n",
        "    return subjects_2019_counts, subjects_2020_counts\n",
        "\n",
        "# Example usage:\n",
        "filepath = ds_split_map[\"test\"]\n",
        "subjects_2019_counts, subjects_2020_counts = get_subject_counts(filepath)\n",
        "print(subjects_2019_counts)\n",
        "print(\"================================================\")\n",
        "print(subjects_2020_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEGZX_XhT1uZ"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def normalize_scores(em_scores, fm_scores, total_riddles):\n",
        "  normalized_em_scores = {model: round((score / total_riddles) * 100, 2) for model, score in em_scores.items()}\n",
        "  normalized_fm_scores = {model: round((score / total_riddles) * 100, 2) for model, score in fm_scores.items()}\n",
        "  return normalized_em_scores, normalized_fm_scores\n",
        "\n",
        "def save_metrics_to_csv(filename, em_scores, fm_scores, total_riddles):\n",
        "  norm_fname = os.path.join(DATA_DIR, f\"Results/{model_name}_eval_em_fm_{filename}_v4.csv\")\n",
        "\n",
        "  # Open the CSV file for writing\n",
        "  with open(norm_fname, 'w', newline='', encoding=\"utf-8\") as norm_csv_file:\n",
        "    writer = csv.writer(norm_csv_file)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow([\"MODEL\", \"EM Score\", \"EM SCORE (%)\", \"FM Score\", \"FM SCORE (%)\", \"NUMBER OF RIDDLES\"])\n",
        "\n",
        "    # Normalize scores and write rows for each model\n",
        "    normalized_em_scores, normalized_fm_scores = normalize_scores(em_scores, fm_scores, total_riddles)\n",
        "    for model in em_scores.keys():\n",
        "      em_score = em_scores[model]\n",
        "      norm_em_score = normalized_em_scores[model]\n",
        "      fm_score = fm_scores[model]\n",
        "      norm_fm_score = normalized_fm_scores[model]\n",
        "      content = [model, em_score, norm_em_score, fm_score, norm_fm_score, total_riddles]\n",
        "      writer.writerow(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvpmCh0Fhheg"
      },
      "outputs": [],
      "source": [
        "total_riddles = 160 # Replace with total number of riddles if different\n",
        "\n",
        "em_scores_19, em_subject_scores_19, fm_scores_19, fm_subject_scores_19 = scores_2019\n",
        "em_scores_20, em_subject_scores_20, fm_scores_20, fm_subject_scores_20 = scores_2020\n",
        "\n",
        "# Save metrics to CSV\n",
        "save_metrics_to_csv('2019', em_scores_19, fm_scores_19, total_riddles-4)\n",
        "save_metrics_to_csv('2020', em_scores_20, fm_scores_20, total_riddles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvTtxhFyPZaZ"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def ps_breakdown_csv(ps_em_scores, ps_fm_scores, counts, year):\n",
        "    csv_filename = os.path.join(DATA_DIR, f\"Results/{model_name}_subject_breakdown_{year}.csv\")\n",
        "\n",
        "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "\n",
        "        # Write the header row\n",
        "        writer.writerow(['Subject', 'EM Score', 'EM Score (%)', 'FM Score', 'FM Score (%)', 'Count'])\n",
        "\n",
        "        for subject, ps_em_score, ps_fm_score in zip(ps_em_scores.keys(), ps_em_scores.values(), ps_fm_scores.values()):\n",
        "            em_percentage = round((ps_em_score[model_name] / counts[subject]) * 100, 2)\n",
        "            fm_percentage = round((ps_fm_score[model_name] / counts[subject]) * 100, 2)\n",
        "\n",
        "            writer.writerow([subject, ps_em_score[model_name], em_percentage, ps_fm_score[model_name], fm_percentage, counts[subject]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sdVoc57Jx67"
      },
      "outputs": [],
      "source": [
        "# 2019\n",
        "ps_breakdown_csv(em_subject_scores_19, fm_subject_scores_19, subjects_2019_counts, \"2019\")\n",
        "\n",
        "# 2020\n",
        "ps_breakdown_csv(em_subject_scores_20, fm_subject_scores_20, subjects_2020_counts, \"2020\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRSe3_IqI9fg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
