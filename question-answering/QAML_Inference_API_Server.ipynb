{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_x38Hklqssx"
      },
      "source": [
        "## Install and Import Essential Packages To Setup Models and API Endpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypeXsY_pjtGI"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "!pip install -q transformers accelerate einops langchain bitsandbytes cohere tiktoken\n",
        "!pip install -q --upgrade openai\n",
        "!pip install -q --upgrade langchain\n",
        "\n",
        "# For API\n",
        "!pip -q install fastapi\n",
        "!pip -q install pyngrok\n",
        "!pip -q install uvicorn\n",
        "!pip -q install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJu5J4TTMcWe",
        "outputId": "dddb3466-bed8-418c-d851-90c97d366b5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive for data persistence (you can usse other storage options if you prefer).\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ETejngFZ4jN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import openai\n",
        "import os\n",
        "import IPython\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV9G4SfVbI-m"
      },
      "outputs": [],
      "source": [
        "# Provide your OpenAI API Key\n",
        "os.environ['OPENAI_API_KEY'] = '' # Replace with your OpenAI API Key, or leave empty if you don't have one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIKbvvlj_V59"
      },
      "outputs": [],
      "source": [
        "import uvicorn\n",
        "import fastapi\n",
        "from pyngrok import ngrok\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEFVKk7jqhtV"
      },
      "source": [
        "## Mistral-7B-Instruct Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1xOgKZOa23Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVIaDCglQAou"
      },
      "outputs": [],
      "source": [
        "# Fetch Mistral-7B-Instruct-v0.1 model\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "\n",
        "pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        use_cache=True,\n",
        "        device_map=\"auto\",\n",
        "        max_length=500,\n",
        "        temperature = 1,\n",
        "        do_sample=False,\n",
        "        #top_k=1,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        torch_dtype=torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHv2TxYdqaKj"
      },
      "source": [
        "## Mistral Setup For Live And Demo Question (Riddle) Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8je6aYe33eDQ"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Function to get mistral answer for live riddles\n",
        "def answer_with_mistral(riddle):\n",
        "  template = \"\"\"  <s>[INST]   You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must first reason through the clues of the given riddle and then provide a short answer. Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "      Here's an example to guide you:\n",
        "      Riddle: You might think i am a rather unstable character because i never stay at one place. However my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time. I can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules. In order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency.\n",
        "      Answer: oscillator\n",
        "\n",
        "      Read the riddle below and provide the three possible correct answers as a json with keys: answer1, answer2, answer3\n",
        "\n",
        "      NOTE: You are allowed to include an answer multiple times if your reasoning shows that it is likely the correct answer. Do not provide any explanations.\n",
        "\n",
        "      Riddle: {riddle}\n",
        "\n",
        "      [/INST] </s>\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"riddle\"])\n",
        "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "  answer = llm_chain.run({\"riddle\":riddle})\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dned1PC75cPf"
      },
      "outputs": [],
      "source": [
        "demo_llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.0})\n",
        "\n",
        "# Function to get mistral answer for demo riddles\n",
        "def demo_qa_mistral_answer(riddle_content):\n",
        "  template = \"\"\" <s>[INST] You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must provide a short answer to a riddle. Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "      Here's an example to guide you:\n",
        "      Riddle: you might think i am a rather unstable character because i never stay at one place, however my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time, i can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules, in order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency,\n",
        "      Answer: oscillator\n",
        "\n",
        "      Read the riddle below and provide the correct answer.\n",
        "\n",
        "     Riddle: {riddle}\n",
        "\n",
        "      [/INST] </s>\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"riddle\"])\n",
        "  falcon_chain = LLMChain(prompt=prompt, llm=demo_llm)\n",
        "  answer = falcon_chain.run({\"riddle\":riddle_content})\n",
        "  return answer.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u59DwmgBqOiE"
      },
      "source": [
        "## ChatGPT Setup For Live Question (Riddle) Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TD9PmLbZYcx"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "# chat mode instance\n",
        "chat = ChatOpenAI(\n",
        "    temperature=1.0)\n",
        "\n",
        "\n",
        "# Function to get ChatGPT answer for live riddle\n",
        "def live_qa_chatgpt_answer(riddle):\n",
        "    template = \"\"\"You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must first reason through the clues of the given riddle and then provide a short answer. Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "      Here's an example to guide you:\n",
        "      Riddle: You might think i am a rather unstable character because i never stay at one place. However my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time. I can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules. In order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency.\n",
        "      Answer: oscillator\n",
        "\n",
        "      Read the riddle below and provide the three possible correct answers as a json with keys: answer1, answer2, answer3\n",
        "\n",
        "      NOTE: You are allowed to include an answer multiple times if your reasoning shows that it is likely the correct answer. Do not provide any explanations.\n",
        "\n",
        "      Riddle: {riddle}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    answer = chat([HumanMessage(content=template.format(riddle=riddle))])\n",
        "    return answer.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Risus5Z2o02I"
      },
      "source": [
        "## Utility Functions And Confidence Modelling Function For Generated Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0BmOC6NLxbW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "import random\n",
        "from glob import glob\n",
        "import time\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Bring log directory from google drive into scope\n",
        "LOG_DIR = \"/content/drive/My Drive/NSMQ AI Project/Competition Logs/QA_LOGS\"  # REPLACE WITH YOUR DESIRED DIRECTORY\n",
        "RIDDLE_ANSWERED_FILE_LIVE = os.path.join(LOG_DIR, \"ans_live_logs.json\")\n",
        "with open(RIDDLE_ANSWERED_FILE_LIVE, 'w') as f:\n",
        "    json.dump({\"Mistral\": '', \"ChatGPT\": ''}, f)\n",
        "\n",
        "def remove_articles(text):\n",
        "    \"\"\"\n",
        "        Remove articles [the|a|an] from `text`\n",
        "\n",
        "        Args:\n",
        "            text: str\n",
        "\n",
        "        Returns:\n",
        "            text with articles removed: str\n",
        "    \"\"\"\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    return re.sub(regex, \" \", text)\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"\n",
        "        Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\n",
        "\n",
        "        Args:\n",
        "            s: (str) string to normalize\n",
        "\n",
        "        Returns:\n",
        "            normalized string: str\n",
        "    \"\"\"\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation.replace(\"/\", \"\"))\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(s)))\n",
        "\n",
        "\n",
        "\n",
        "def model_answer_confidence(model, threshold, chunk_num, model_answer, is_start_of_riddle):\n",
        "    \"\"\"\n",
        "        Calculate confidence scores for model-generated answers.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): The name of the language model.\n",
        "            confidence_threshold (float): The confidence score threshold for accepting answers.\n",
        "            chunk_num (int): The chunk number for the riddle.\n",
        "            model_output (list): A list of answers generated by the model.\n",
        "            is_start_of_riddle (bool): Indicates if this is the start of a new riddle.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing a list of answers and their associated confidence scores.\n",
        "\n",
        "        This function calculates confidence scores for answers generated by a language model\n",
        "        and returns a tuple of answers and their confidence scores.\n",
        "    \"\"\"\n",
        "    cur_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")  # Get the current time\n",
        "\n",
        "    if is_start_of_riddle or chunk_num == 1:\n",
        "        # Create a new JSON log file if it is a new riddle.\n",
        "        print(f\"Is Start of Riddle: {is_start_of_riddle}\\t Clue Count: {chunk_num}\")\n",
        "        filename = os.path.join(LOG_DIR, f\"{model}_log_{cur_time}.json\")\n",
        "        answer_counts = {}\n",
        "    else:\n",
        "        # Find the most recent generated JSON log file, if it is not a new riddle.\n",
        "        log_files = glob(os.path.join(LOG_DIR, f\"{model}_log_*.json\"))\n",
        "        if log_files:\n",
        "            # Sort the log files by modification time to get the most recent one.\n",
        "            log_files.sort(key=os.path.getmtime, reverse=True)\n",
        "            filename = log_files[0]\n",
        "            with open(filename, 'r') as f:\n",
        "                logged_data = json.load(f)\n",
        "                answer_counts = logged_data[\"answer_counts\"]\n",
        "                print(\"Loaded Answer Counts Dictionary:\", answer_counts)\n",
        "        else:\n",
        "            # If no log files exist, create a new one.\n",
        "            filename = os.path.join(LOG_DIR, f\"{model}_log_{cur_time}.json\")\n",
        "            answer_counts = {}\n",
        "\n",
        "    # Update answer_counts based on the model answer and chunk number\n",
        "    for ans in model_answer:\n",
        "        ans = remove_articles(normalize_text(ans).replace('\"', '')).strip()\n",
        "        answer_counts[ans] = answer_counts.get(ans, 0) + int(chunk_num)\n",
        "\n",
        "    answer_counts[''] = 0\n",
        "\n",
        "    print(\"Answer Counts Dictionary:\", answer_counts)\n",
        "\n",
        "    # Find the top answers and write answer_counts to the JSON log file\n",
        "    top_count = max(answer_counts.values())\n",
        "    top_answers = [ans for ans, count in answer_counts.items() if count == top_count]\n",
        "\n",
        "    top_answer = \"\"\n",
        "\n",
        "    if top_count >= threshold:\n",
        "        top_answer = random.choice(top_answers)\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        data_to_save = {\n",
        "            \"answer_counts\": answer_counts,\n",
        "            \"top_answer\": (top_answer, top_count)\n",
        "        }\n",
        "        json.dump(data_to_save, f)\n",
        "\n",
        "        print(\"Saved Data:\", data_to_save)\n",
        "\n",
        "    return top_answer, top_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsdNDood85Wz"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import json\n",
        "\n",
        "\n",
        "def preprocess_model_output(model_output):\n",
        "    \"\"\"\n",
        "      Preprocess the output from a language model.\n",
        "\n",
        "      Args:\n",
        "          model_output (dict): The raw output from a language model.\n",
        "\n",
        "      Returns:\n",
        "          dict: The preprocessed model output.\n",
        "\n",
        "      This function performs any necessary preprocessing on the model's output\n",
        "      to convert the model's output into a json for the next steps.\n",
        "    \"\"\"\n",
        "    if isinstance(model_output, dict):\n",
        "        return model_output\n",
        "\n",
        "    # Convert model output to string\n",
        "    model_output = str(model_output).replace(\"\\n\", '').strip()\n",
        "\n",
        "    # Remove all text that are not enclosed in '{' and '}'\n",
        "    pattern = r'{.*?}'\n",
        "    m = re.search(pattern, model_output)\n",
        "    model_output = m.group(0)\n",
        "\n",
        "    # Remove ` characters if any\n",
        "    model_output = model_output.replace('```', '').replace('json', '')\n",
        "\n",
        "    # Surround model output in curly braces if it isn't already.\n",
        "    if not model_output.startswith(\"{\") or not model_output.endswith(\"}\"):\n",
        "        model_output = '{' + model_output + '}'\n",
        "\n",
        "    # Replace null in quotes and replace with none\n",
        "    model_output = model_output.replace(\": null\", \"'null'\")\n",
        "    print(\"State of Model Output:\", model_output)\n",
        "\n",
        "    # Try converting answer to json\n",
        "    try:\n",
        "        json_data = json.loads(model_output)\n",
        "        return json_data\n",
        "    except (SyntaxError, ValueError):\n",
        "        print(\"SOMETHING WENT WRONG!\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmgfhWdRPQ97"
      },
      "source": [
        "## FastAPI Endpoints for Live and Demo Question-Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOYahAR5AdHn"
      },
      "outputs": [],
      "source": [
        "class DemoInputText(BaseModel):\n",
        "    \"\"\"\n",
        "        Data model for demo input containing text.\n",
        "\n",
        "        Attributes:\n",
        "            text (str): The text content for the demo input.\n",
        "    \"\"\"\n",
        "    text: str\n",
        "\n",
        "\n",
        "class LiveInputText(BaseModel):\n",
        "    \"\"\"\n",
        "        Data model for live input containing clues and metadata.\n",
        "\n",
        "        Attributes:\n",
        "            clues (str): The clues or questions for the live input.\n",
        "            is_start_of_riddle (bool): Indicates if this is the start of a new riddle.\n",
        "            is_end_of_riddle (bool): Indicates if this is the end of a riddle.\n",
        "            clue_count (int): The count of clues provided.\n",
        "    \"\"\"\n",
        "    clues: str\n",
        "    is_start_of_riddle: bool = False\n",
        "    is_end_of_riddle: bool = False\n",
        "    clue_count: int = 0\n",
        "\n",
        "\n",
        "class LiveDemoInputText(BaseModel):\n",
        "    \"\"\"\n",
        "        Data model for live demo input containing clues, metadata, and a threshold.\n",
        "\n",
        "        Attributes:\n",
        "            clues (str): The clues or questions for the live demo input.\n",
        "            is_start_of_riddle (bool): Indicates if this is the start of a new riddle.\n",
        "            is_end_of_riddle (bool): Indicates if this is the end of a riddle.\n",
        "            clue_count (int): The count of clues provided.\n",
        "            threshold (int): The threshold value, set to 4 by default.\n",
        "    \"\"\"\n",
        "    clues: str\n",
        "    is_start_of_riddle: bool = False\n",
        "    is_end_of_riddle: bool = False\n",
        "    clue_count: int = 0\n",
        "    threshold = 4\n",
        "\n",
        "class OutputText(BaseModel):\n",
        "    \"\"\"\n",
        "        Data model for output text, including Mistral and ChatGPT responses.\n",
        "\n",
        "        Attributes:\n",
        "            mistral (str): The response from the Mistral model.\n",
        "            chatGPT (str, optional): The response from the ChatGPT model, which is optional.\n",
        "    \"\"\"\n",
        "    mistral: str\n",
        "    chatGPT: str = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mQ1LfD5NCuJ"
      },
      "outputs": [],
      "source": [
        "app = fastapi.FastAPI()\n",
        "\n",
        "\n",
        "def filter_answers(ans_data, confidence_threshold, is_end_of_riddle):\n",
        "    \"\"\"\n",
        "        Filter answers based on confidence threshold and riddle completion status.\n",
        "\n",
        "        Args:\n",
        "            ans_data (tuple): A tuple containing answer data, where the first element is the answer\n",
        "                and the second element is the confidence score.\n",
        "            confidence_threshold (float): The confidence score threshold for accepting answers.\n",
        "            is_end_of_riddle (bool): Indicates whether this is the end of the riddle.\n",
        "\n",
        "        Returns:\n",
        "            str: The filtered answer if the confidence score is above the threshold or if it's\n",
        "            the end of the riddle; otherwise, an empty string.\n",
        "    \"\"\"\n",
        "    confidence = ans_data[1]\n",
        "    if int(confidence) >= confidence_threshold or is_end_of_riddle:\n",
        "      return ans_data[0]\n",
        "\n",
        "    return ''\n",
        "\n",
        "\n",
        "def load_riddle_answered_log(is_start_of_riddle=True, chunk_num=1):\n",
        "    \"\"\"\n",
        "        Load previously answered riddle data from a file.\n",
        "\n",
        "        Args:\n",
        "            is_start_of_riddle (bool): Indicates if this is the start of a new riddle.\n",
        "            chunk_num (int): The chunk number for the riddle.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing previously answered riddles, with keys 'mistral' and 'chatGPT'.\n",
        "            If it's the start of a riddle, an empty dictionary is returned.\n",
        "    \"\"\"\n",
        "    if is_start_of_riddle == True:\n",
        "      return {\"mistral\": '' , \"chatGPT\": ''}\n",
        "    else:\n",
        "      if os.path.exists(RIDDLE_ANSWERED_FILE_LIVE):\n",
        "        with open(RIDDLE_ANSWERED_FILE_LIVE, \"r\") as file:\n",
        "          return json.load(file)\n",
        "      else:\n",
        "        return {\"mistral\": '', \"chatGPT\": ''}\n",
        "\n",
        "def save_riddle_answered_log(data):\n",
        "    \"\"\"\n",
        "        Save riddle answer data to a file.\n",
        "\n",
        "        Args:\n",
        "            data (dict): A dictionary containing riddle answer data to be saved.\n",
        "\n",
        "        This function saves the answer data to a file for later retrieval.\n",
        "    \"\"\"\n",
        "    with open(RIDDLE_ANSWERED_FILE_LIVE, \"w\") as file:\n",
        "        json.dump(data, file)\n",
        "\n",
        "\n",
        "@app.get(\"/live_qa\", response_model=OutputText)\n",
        "def live_answer(input_data: LiveInputText):\n",
        "    \"\"\"\n",
        "        Perform live answering for a given input containing clues and metadata.\n",
        "\n",
        "        Args:\n",
        "            input_data (LiveInputText): Input data containing clues and metadata (is_start_of_riddle, chunk_num and is_end_of_riddle).\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing answers generated using Mistral and ChatGPT.\n",
        "                Keys are 'mistral' and 'chatGPT', and values are lists of answers.\n",
        "\n",
        "        This function loads previously computed answers, uses the Mistral model to generate answers\n",
        "        if necessary, and filters and stores the results. It returns a dictionary with the answers.\n",
        "    \"\"\"\n",
        "    ct = 10.0  # ct represents the confidence threshold.\n",
        "    chunk_num = input_data.clue_count\n",
        "    is_start_of_riddle = input_data.is_start_of_riddle\n",
        "    is_end_of_riddle = input_data.is_end_of_riddle\n",
        "\n",
        "\n",
        "    # Load data previously computed answers\n",
        "    answer_file = load_riddle_answered_log(is_start_of_riddle, chunk_num)\n",
        "\n",
        "    if answer_file['mistral'] == '' and chunk_num != 0:\n",
        "        # Send clues to mistral-7b and get answer if we haven't answered the riddle yet\n",
        "        mistral_output = answer_with_mistral(riddle=input_data.clues)\n",
        "        mistral_output = preprocess_model_output(mistral_output)\n",
        "\n",
        "        #Put answers in a list\n",
        "        if mistral_output is not None:\n",
        "            mistral_output = [mistral_output[key] for key in mistral_output.keys()]\n",
        "        else:\n",
        "          mistral_output = ['']\n",
        "          print(\"Failed to convert Mistral response to dict/json\")\n",
        "        mistral_ans_data = model_answer_confidence(\"Mistral\", ct, chunk_num, mistral_output, is_start_of_riddle)\n",
        "        mistral_final_ans = filter_answers(mistral_ans_data, ct, is_end_of_riddle)\n",
        "    else:\n",
        "      # If we've already answered on previous clues, load that answer instead\n",
        "      mistral_final_ans = answer_file['mistral']\n",
        "\n",
        "    if answer_file['chatGPT'] == '' and chunk_num != 0:\n",
        "        # Send clues to ChatGPT and get answer if we havent't answered the riddle yet\n",
        "        if os.environ['OPENAI_API_KEY'] != '':\n",
        "            chatGPT_output = live_qa_chatgpt_answer(input_data.clues)\n",
        "        else:\n",
        "            chatGPT_output = {\"answer1\": ''}\n",
        "        chatGPT_output = preprocess_model_output(chatGPT_output)\n",
        "        # Put answers in a list\n",
        "        if chatGPT_output is not None:\n",
        "            chatGPT_output = [chatGPT_output[key] for key in chatGPT_output.keys()]\n",
        "        else:\n",
        "          chatGPT_output = ['',]\n",
        "          print(\"Failed to convert ChatGPT response to dict/json\")\n",
        "        chatGPT_ans_data = model_answer_confidence(\"ChatGPT\", ct, chunk_num, chatGPT_output, is_start_of_riddle)\n",
        "        chatgpt_final_ans = filter_answers(chatGPT_ans_data, ct, is_end_of_riddle)\n",
        "    else:\n",
        "      # If we've already answered on previous clues, load that answer instead\n",
        "      chatgpt_final_ans = answer_file['chatGPT']\n",
        "\n",
        "\n",
        "    answers = {\n",
        "        \"mistral\": mistral_final_ans,\n",
        "        \"chatGPT\": chatgpt_final_ans\n",
        "    }\n",
        "\n",
        "    # Save answers data to file. This is to ensure that once we've returned an answer\n",
        "    # for a riddle, we no longer do inference for subsequent clues till the next riddle.\n",
        "    save_riddle_answered_log(answers)\n",
        "\n",
        "    return answers\n",
        "\n",
        "\n",
        "@app.get(\"/live_demo_qa\", response_model=OutputText)\n",
        "def live_demo_answer(input_data: LiveDemoInputText):\n",
        "    \"\"\"\n",
        "        Perform live answering for a given input containing clues and metadata.\n",
        "\n",
        "        Args:\n",
        "            input_data (LiveInputText): Input data containing clues and metadata (is_start_of_riddle, chunk_num and is_end_of_riddle).\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing answers generated using Mistral and ChatGPT.\n",
        "                Keys are 'mistral' and 'chatGPT', and values are lists of answers.\n",
        "\n",
        "        This function loads previously computed answers, uses the Mistral model to generate answers\n",
        "        if necessary, and filters and stores the results. It returns a dictionary with the answers.\n",
        "    \"\"\"\n",
        "    ct = 10.0  # ct represents the confidence threshold.\n",
        "    chunk_num = input_data.clue_count\n",
        "    is_start_of_riddle = input_data.is_start_of_riddle\n",
        "    is_end_of_riddle = input_data.is_end_of_riddle\n",
        "\n",
        "\n",
        "    # Load data previously computed answers\n",
        "    answer_file = load_riddle_answered_log(is_start_of_riddle, chunk_num)\n",
        "\n",
        "    if answer_file['mistral'] == '' and chunk_num != 0:\n",
        "        # Send clues to mistral-7b and get answer if we haven't answered the riddle yet\n",
        "        mistral_output = answer_with_mistral(riddle=input_data.clues)\n",
        "        mistral_output = preprocess_model_output(mistral_output)\n",
        "\n",
        "        #Put answers in a list\n",
        "        if mistral_output is not None:\n",
        "            mistral_output = [mistral_output[key] for key in mistral_output.keys()]\n",
        "        else:\n",
        "          mistral_output = ['']\n",
        "          print(\"Failed to convert Mistral response to dict/json\")\n",
        "        mistral_ans_data = model_answer_confidence(\"Mistral\", ct, chunk_num, mistral_output, is_start_of_riddle)\n",
        "        mistral_final_ans = filter_answers(mistral_ans_data, ct, is_end_of_riddle)\n",
        "    else:\n",
        "      # If we've already answered on previous clues, load that answer instead\n",
        "      mistral_final_ans = answer_file['mistral']\n",
        "\n",
        "    if answer_file['chatGPT'] == '' and chunk_num != 0:\n",
        "        # Send clues to ChatGPT and get answer if we havent't answered the riddle yet\n",
        "        if os.environ['OPENAI_API_KEY'] != '':\n",
        "            chatGPT_output = live_qa_chatgpt_answer(input_data.clues)\n",
        "        else:\n",
        "            chatGPT_output = {\"answer1\": ''}\n",
        "        chatGPT_output = preprocess_model_output(chatGPT_output)\n",
        "        # Put answers in a list\n",
        "        if chatGPT_output is not None:\n",
        "            chatGPT_output = [chatGPT_output[key] for key in chatGPT_output.keys()]\n",
        "        else:\n",
        "          chatGPT_output = ['',]\n",
        "          print(\"Failed to convert ChatGPT response to dict/json\")\n",
        "        chatGPT_ans_data = model_answer_confidence(\"ChatGPT\", ct, chunk_num, chatGPT_output, is_start_of_riddle)\n",
        "        chatgpt_final_ans = filter_answers(chatGPT_ans_data, ct, is_end_of_riddle)\n",
        "    else:\n",
        "      # If we've already answered on previous clues, load that answer instead\n",
        "      chatgpt_final_ans = answer_file['chatGPT']\n",
        "\n",
        "\n",
        "    answers = {\n",
        "        \"mistral\": mistral_final_ans,\n",
        "        \"chatGPT\": chatgpt_final_ans\n",
        "    }\n",
        "\n",
        "    # Save answers data to file. This is to ensure that once we've returned an answer\n",
        "    # for a riddle, we no longer do inference for subsequent clues till the next riddle.\n",
        "    save_riddle_answered_log(answers)\n",
        "\n",
        "    return answers\n",
        "\n",
        "@app.get('/demo_qa', response_model=OutputText)\n",
        "def demo_answer(input_data: DemoInputText):\n",
        "    \"\"\"\n",
        "        Perform a demo question-answering using the Mistral model.\n",
        "\n",
        "        Args:\n",
        "            input_data (DemoInputText): Input data containing the riddle content.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing answers generated using the Mistral model.\n",
        "\n",
        "        This function takes the riddle content as input, sends it to the Mistral model, and returns\n",
        "        the model's generated answers in a dictionary with the 'mistral' key.\n",
        "    \"\"\"\n",
        "    riddle_content = input_data.text\n",
        "    mistral_ans = demo_qa_mistral_answer(riddle_content)\n",
        "\n",
        "    answers = {\n",
        "        \"mistral\": mistral_ans\n",
        "    }\n",
        "    return answers\n",
        "\n",
        "\n",
        "@app.get('/test_qa', response_model=OutputText)\n",
        "def test():\n",
        "    # Test that endpoint is up and running.\n",
        "    return {\"mistral\": \"Hello from Mistral!\", \"chatGPT\": \"Hello from ChatGPT!\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooh6bkk-gHde"
      },
      "source": [
        "## Creating A Public Tunnel for FastAPI App With Ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R17_xUCgnVtJ"
      },
      "outputs": [],
      "source": [
        "# Add your ngrok auth token\n",
        "!ngrok config add-authtoken #[YOU NGROK AUTH TOKEN]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMlBe-Po07ug"
      },
      "outputs": [],
      "source": [
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "uvicorn.run(app, port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
