{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brilla-ai/brilla-ai/blob/kojomensahonums-starter-notebook-upload/STT_Starter_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AfricAIED 2024 Hackathon\n",
        "This notebook is designed to help you get started with using speech transcription models for your project. In this notebook we show how to import and transcribe an audio using distil-whisper, a lightweight variant of the Whisper model.<br>\n",
        "\n",
        "The audios present in this notebook are available in the \"starter notebook audios\" folder. Download the audios and upload them into Google Colab. The reference paths should then be the same as those present already. Happy transcribing! ðŸ˜€"
      ],
      "metadata": {
        "id": "eyu93xTSnKEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About the Speech Transcription Model"
      ],
      "metadata": {
        "id": "dj-z4DVW-25c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distil-whisper collection currently has 4 different model sizes. Performance quality increases from the smallest to the largest. Arranged in order of increasing size, the models are as follows:\n",
        "\n",
        "\n",
        "*   distil-small.en\n",
        "*   distill-medium.en\n",
        "*   distil-large-v2\n",
        "*   distil-large-v3\n",
        "\n",
        "Distil-large-v2 is used in this notebook. You can try the different models out!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TCU5LyDAi0YQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "9kHfQKgy6FKW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG0fM3XCa451"
      },
      "outputs": [],
      "source": [
        "# Install and import required libraries\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer\n",
        "!pip install tabulate\n",
        "!pip install pydub\n",
        "!pip install -q torchaudio\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade transformers accelerate datasets[audio]\n",
        "import torch\n",
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import IPython.display as ipd\n",
        "from IPython.display import Audio, clear_output\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zyg1JLiFIJA"
      },
      "source": [
        "#### Short-form Transcription"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Short-form transcription is the process of transcribing audio samples that are less than 30-seconds long, which is the maximum receptive field of the Whisper models. This means the entire audio clip can be processed in one go without the need for chunking."
      ],
      "metadata": {
        "id": "WcxyRV2cnqeC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdFj2n8Eh8Ks"
      },
      "source": [
        "Import and play audio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_audio = AudioSegment.from_file(r\"/content/audio_riddle_short_form.wav\")\n",
        "sample_audio\n"
      ],
      "metadata": {
        "id": "9Uyq3zNOygKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "HrSQMJYTAYmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model_id = \"distil-whisper/distil-large-v2\" # Test out the different model sizes here\n",
        "\n",
        "model_short = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model_short.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model_short,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=128,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "P1vIQKv4kozW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To transcribe a local audio file, simply pass the path to your audio file when you call the pipeline."
      ],
      "metadata": {
        "id": "P4gAZia_n5uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe audio\n",
        "result = pipe(r\"/content/audio_riddle_short_form.wav\")\n",
        "\n",
        "# Print transcript\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "gBgmmhkXqHOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KqSfNhco6xcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC8P1dFqFPJk"
      },
      "source": [
        "#### Chunked Long-form transcription"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This algorithm should be used when a single large audio file is being transcribed and the fastest possible inference is required."
      ],
      "metadata": {
        "id": "g6NI80vXouaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and play audio"
      ],
      "metadata": {
        "id": "bVJLZ_R9wAMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_audio_2 = AudioSegment.from_file(r\"/content/audio_riddle_long_form.wav\")\n",
        "sample_audio_2\n"
      ],
      "metadata": {
        "id": "OrVrwy5l7UaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "qbq8xHUyAfV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mteaZ4LlFTK9"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model_id = \"distil-whisper/distil-large-v2\"  # Test out the different model sizes here\n",
        "\n",
        "model_long = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model_long.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "pipe_long = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model_long,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=128,\n",
        "    chunk_length_s=25, # difference\n",
        "    batch_size=16, # difference\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To transcribe a local audio file, simply pass the path to your audio file when you call the pipeline."
      ],
      "metadata": {
        "id": "u8AlKsj8pFhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe audio\n",
        "result2 = pipe_long(r\"/content/audio_riddle_long_form.wav\")\n",
        "\n",
        "# Print transcript\n",
        "print(result2[\"text\"])"
      ],
      "metadata": {
        "id": "i-Vw8lwyljZN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}